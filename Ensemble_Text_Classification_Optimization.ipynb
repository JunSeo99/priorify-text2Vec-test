{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ğŸš€ ì•™ìƒë¸” ê¸°ë²•ì„ í™œìš©í•œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ìµœì í™”\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
        "1. **ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: ë‹¤ì–‘í•œ ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ìµœì ê°’ íƒìƒ‰\n",
        "2. **V3 ì•™ìƒë¸” ì ìš©**: NER + í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”©ì— ì•™ìƒë¸” ê¸°ë²• ì ìš©\n",
        "3. **V4 íŒŒì¸íŠœë‹ + ì•™ìƒë¸”**: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì— ì•™ìƒë¸” ê¸°ë²• ì ìš©\n",
        "\n",
        "## ğŸ“Š ë°ì´í„° ì‚¬ìš©ëŸ‰\n",
        "- **ì „ì²´ ë°ì´í„° í™œìš©**: 1,447ê°œì˜ ì‹¤ì œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©\n",
        "- **ì•™ìƒë¸” í‰ê°€**: ì „ì²´ ë°ì´í„°ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •\n",
        "- **íŒŒì¸íŠœë‹**: 800ê°œ ë°ì´í„°ë¡œ 3 ì—í¬í¬ í•™ìŠµ\n",
        "- **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: 10ê°€ì§€ ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸\n",
        "\n",
        "## ğŸ“‹ ì‹¤í–‰ ì „ ì¤€ë¹„ì‚¬í•­\n",
        "1. ë°ì´í„°ì…‹ ì—…ë¡œë“œ (ì•„ë˜ ì…€ì—ì„œ ì•ˆë‚´)\n",
        "2. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •\n",
        "3. í•˜ë“œì›¨ì–´ ê°€ì† ì„¤ì • (GPU ê¶Œì¥)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ í™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install sentence-transformers transformers torch scikit-learn pandas numpy tqdm matplotlib seaborn\n",
        "\n",
        "# í•˜ë“œì›¨ì–´ í™•ì¸\n",
        "import torch\n",
        "print(f\"ğŸ” í•˜ë“œì›¨ì–´ ì •ë³´:\")\n",
        "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU ì¥ì¹˜: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "print(f\"CPU ì½”ì–´ ìˆ˜: {torch.get_num_threads()}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ¯ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}\")\n",
        "\n",
        "# ê¸°ë³¸ ì„¤ì •\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# wandb ë¹„í™œì„±í™” (ë¡œê·¸ì¸ ì—†ì´ ì‹¤í–‰í•˜ê¸° ìœ„í•¨)\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "print(\"ğŸ”• wandb ì¶”ì  ë¹„í™œì„±í™”ë¨ (ë¡œê·¸ì¸ ë¶ˆí•„ìš”)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ“ ë°ì´í„°ì…‹ ì¤€ë¹„ ë°©ë²•\n",
        "\n",
        "### ë°©ë²• 1: ì§ì ‘ ì—…ë¡œë“œ\n",
        "1. ì™¼ìª½ íŒ¨ë„ì˜ íŒŒì¼ ì•„ì´ì½˜ í´ë¦­\n",
        "2. `data.csv` íŒŒì¼ì„ ë“œë˜ê·¸ ì•¤ ë“œë¡­ìœ¼ë¡œ ì—…ë¡œë“œ\n",
        "\n",
        "### ë°©ë²• 2: Google Drive ì—°ë™\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# íŒŒì¼ ê²½ë¡œ: /content/drive/MyDrive/your_file.csv\n",
        "```\n",
        "\n",
        "### ë°©ë²• 3: ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\n",
        "ì•„ë˜ ì…€ì—ì„œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_sample_data():\n",
        "    \"\"\"í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "    sample_data = {\n",
        "        'í•™ì—…': [\n",
        "            'ìˆ˜í•™ ê³¼ì œ ì œì¶œí•˜ê¸°', 'ì˜ì–´ ì‹œí—˜ ê³µë¶€í•˜ê¸°', 'ë…¼ë¬¸ ì‘ì„±í•˜ê³  ë°œí‘œí•˜ê¸°', \n",
        "            'í”„ë¡œì íŠ¸ íŒ€ íšŒì˜ ì°¸ì„í•˜ê¸°', 'ì‹¤í—˜ì‹¤ì—ì„œ ì—°êµ¬í•˜ê¸°', 'ë„ì„œê´€ì—ì„œ ìë£Œ ì¡°ì‚¬í•˜ê¸°'\n",
        "        ],\n",
        "        'ì—…ë¬´': [\n",
        "            'íšŒì˜ ì°¸ì„í•˜ê³  ë³´ê³ ì„œ ì‘ì„±í•˜ê¸°', 'í´ë¼ì´ì–¸íŠ¸ì™€ ë¯¸íŒ…í•˜ê¸°', 'í”„ë ˆì  í…Œì´ì…˜ ì¤€ë¹„í•˜ê¸°',\n",
        "            'ë©”ì¼ í™•ì¸í•˜ê³  ë‹µë³€í•˜ê¸°', 'ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ê¸°íší•˜ê¸°', 'íŒ€ì›ë“¤ê³¼ í˜‘ì—…í•˜ê¸°'\n",
        "        ],\n",
        "        'ê±´ê°•': [\n",
        "            'í—¬ìŠ¤ì¥ì—ì„œ ìš´ë™í•˜ê¸°', 'ë³‘ì›ì—ì„œ ê±´ê°• ê²€ì§„ ë°›ê¸°', 'ìš”ê°€ í´ë˜ìŠ¤ ì°¸ì—¬í•˜ê¸°',\n",
        "            'ì‚°ì±…í•˜ë©° ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œí•˜ê¸°', 'ì¶©ë¶„í•œ ìˆ˜ë©´ ì·¨í•˜ê¸°', 'ê±´ê°•í•œ ì‹ë‹¨ ê´€ë¦¬í•˜ê¸°'\n",
        "        ],\n",
        "        'ê²½ì œ': [\n",
        "            'ê°€ê³„ë¶€ ì •ë¦¬í•˜ê³  ì˜ˆì‚° ê´€ë¦¬í•˜ê¸°', 'íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ ì ê²€í•˜ê¸°', 'ì ê¸ˆ ë„£ê¸°',\n",
        "            'ë¶€ë™ì‚° ì‹œì¥ ì¡°ì‚¬í•˜ê¸°', 'ì¬í…Œí¬ ê°•ì˜ ë“£ê¸°', 'ìš©ëˆ ê¸°ì…ì¥ ì‘ì„±í•˜ê¸°'\n",
        "        ],\n",
        "        'ì¹œëª©': [\n",
        "            'ì¹œêµ¬ë“¤ê³¼ ì¹´í˜ì—ì„œ ë§Œë‚˜ê¸°', 'ë™ì°½íšŒ ì°¸ì„í•˜ê¸°', 'ê°€ì¡±ê³¼ ì €ë… ì‹ì‚¬í•˜ê¸°',\n",
        "            'ë™ë£Œë“¤ê³¼ íšŒì‹í•˜ê¸°', 'ì»¤ë®¤ë‹ˆí‹° ëª¨ì„ ì°¸ì—¬í•˜ê¸°', 'ìƒˆë¡œìš´ ì‚¬ëŒë“¤ê³¼ ë„¤íŠ¸ì›Œí‚¹í•˜ê¸°'\n",
        "        ],\n",
        "        'ì·¨ë¯¸': [\n",
        "            'ë…ì„œí•˜ë©° ì—¬ê°€ ì‹œê°„ ë³´ë‚´ê¸°', 'ì˜í™” ê´€ëŒí•˜ê¸°', 'ìš”ë¦¬ ë ˆì‹œí”¼ ë„ì „í•˜ê¸°',\n",
        "            'ì—¬í–‰ ê³„íš ì„¸ìš°ê¸°', 'ì·¨ë¯¸ í´ë˜ìŠ¤ ìˆ˜ê°•í•˜ê¸°', 'ì˜¨ë¼ì¸ ê²Œì„ ì¦ê¸°ê¸°'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    # ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "    rows = []\n",
        "    for category, titles in sample_data.items():\n",
        "        for title in titles:\n",
        "            rows.append({'title': title, 'categories': category})\n",
        "    \n",
        "    # ì¶”ê°€ ëœë¤ ë°ì´í„° ìƒì„±\n",
        "    categories = list(sample_data.keys())\n",
        "    for i in range(100):  # 100ê°œ ì¶”ê°€ ìƒ˜í”Œ\n",
        "        category = np.random.choice(categories)\n",
        "        title = f\"{category} ê´€ë ¨ í™œë™ {i+1}\"\n",
        "        rows.append({'title': title, 'categories': category})\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv('sample_data.csv', index=False)\n",
        "    print(f\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê°œ í•­ëª©\")\n",
        "    return df\n",
        "\n",
        "# ì‹¤ì œ ë°ì´í„° íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸\n",
        "DATA_PATH = 'data/data.csv'  # ì‹¤ì œ ë°ì´í„° ê²½ë¡œ\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    # ëŒ€ì²´ ê²½ë¡œ í™•ì¸\n",
        "    if os.path.exists('data.csv'):\n",
        "        DATA_PATH = 'data.csv'\n",
        "    else:\n",
        "        print(\"âŒ data.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        print(\"ğŸ”„ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
        "        df = create_sample_data()\n",
        "        DATA_PATH = 'sample_data.csv'\n",
        "\n",
        "if 'df' not in locals():\n",
        "    print(f\"âœ… {DATA_PATH} íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)}ê°œ í•­ëª© ë¡œë“œ\")\n",
        "\n",
        "print(f\"ğŸ“Š ë°ì´í„° ì •ë³´: {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì—´\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
        "from sentence_transformers import SentenceTransformer, util, InputExample, losses, evaluation\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "\n",
        "# ëª¨ë¸ ë° ì„¤ì •\n",
        "BASE_MODEL_NAME = 'jhgan/ko-sroberta-multitask'\n",
        "NER_MODEL_NAME = 'Leo97/KoELECTRA-small-v3-modu-ner'\n",
        "\n",
        "# ì¹´í…Œê³ ë¦¬ ì •ì˜ (í‚¤ì›Œë“œ ê¸°ë°˜)\n",
        "CATEGORIES_DEFINITIONS = {\n",
        "    'í•™ì—…': [\n",
        "        'ê³¼ì œ ì‘ì„±í•˜ê³  ì œì¶œí•˜ê¸°', 'ì‹œí—˜ ì¤€ë¹„í•˜ê³  ê³µë¶€í•˜ê¸°', 'ì—°êµ¬ í™œë™í•˜ê³  ë…¼ë¬¸ ì“°ê¸°',\n",
        "        'ì‹¤í—˜ì‹¤ì—ì„œ ì—°êµ¬í•˜ê¸°', 'ì„¸ë¯¸ë‚˜ ì°¸ì„í•˜ê³  ë°œí‘œí•˜ê¸°', 'ë„ì„œê´€ì—ì„œ ìë£Œ ì°¾ê¸°',\n",
        "        'í”„ë¡œì íŠ¸ íŒ€ì›ê³¼ í˜‘ì—…í•˜ê¸°', 'ì˜¨ë¼ì¸ ê°•ì˜ ìˆ˜ê°•í•˜ê¸°'\n",
        "    ],\n",
        "    'ì—…ë¬´': [\n",
        "        'íšŒì˜ ì°¸ì„í•˜ê³  ì˜ê²¬ ì œì‹œí•˜ê¸°', 'ë³´ê³ ì„œ ì‘ì„±í•˜ê³  ê²€í† í•˜ê¸°', 'í´ë¼ì´ì–¸íŠ¸ì™€ ë¯¸íŒ…í•˜ê¸°',\n",
        "        'í”„ë ˆì  í…Œì´ì…˜ ì¤€ë¹„í•˜ê³  ë°œí‘œí•˜ê¸°', 'ì—…ë¬´ ë©”ì¼ í™•ì¸í•˜ê³  ë‹µë³€í•˜ê¸°', 'ìƒˆ í”„ë¡œì íŠ¸ ê¸°íší•˜ê¸°',\n",
        "        'ë™ë£Œë“¤ê³¼ í˜‘ì—…í•˜ê³  ì†Œí†µí•˜ê¸°', 'ì‹¤ì  ë¶„ì„í•˜ê³  ê°œì„ í•˜ê¸°'\n",
        "    ],\n",
        "    'ê±´ê°•': [\n",
        "        'í—¬ìŠ¤ì¥ì—ì„œ ìš´ë™í•˜ê³  ê·¼ë ¥ ê¸°ë¥´ê¸°', 'ë³‘ì›ì—ì„œ ê±´ê°• ê²€ì§„ ë°›ê¸°', 'ìš”ê°€ë‚˜ í•„ë¼í…ŒìŠ¤ í•˜ê¸°',\n",
        "        'ì‚°ì±…í•˜ë©° ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œí•˜ê¸°', 'ì¶©ë¶„í•œ ìˆ˜ë©´ ì·¨í•˜ê³  ì»¨ë””ì…˜ ê´€ë¦¬í•˜ê¸°', 'ê±´ê°•í•œ ì‹ë‹¨ ê³„íší•˜ê³  ì‹¤ì²œí•˜ê¸°',\n",
        "        'ê¸ˆì—°ê³¼ ê¸ˆì£¼ ì‹¤ì²œí•˜ê¸°', 'ì •ê¸°ì ì¸ ê±´ê°• ê´€ë¦¬í•˜ê¸°'\n",
        "    ],\n",
        "    'ê²½ì œ': [\n",
        "        'ê°€ê³„ë¶€ ì •ë¦¬í•˜ê³  ì§€ì¶œ ê´€ë¦¬í•˜ê¸°', 'íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±í•˜ê³  ê´€ë¦¬í•˜ê¸°', 'ì ê¸ˆê³¼ ì˜ˆê¸ˆ ë„£ê¸°',\n",
        "        'ë¶€ë™ì‚° ì‹œì¥ ì¡°ì‚¬í•˜ê³  ë¶„ì„í•˜ê¸°', 'ì¬í…Œí¬ ê³µë¶€í•˜ê³  ì‹¤ì²œí•˜ê¸°', 'ìš©ëˆ ê¸°ì…ì¥ ì‘ì„±í•˜ê¸°',\n",
        "        'ë³´í—˜ ìƒí’ˆ ë¹„êµí•˜ê³  ê°€ì…í•˜ê¸°', 'ì„¸ê¸ˆ ì‹ ê³ í•˜ê³  í™˜ê¸‰ë°›ê¸°'\n",
        "    ],\n",
        "    'ì¹œëª©': [\n",
        "        'ì¹œêµ¬ë“¤ê³¼ ë§Œë‚˜ì„œ ëŒ€í™”í•˜ê¸°', 'ë™ì°½íšŒë‚˜ ëª¨ì„ ì°¸ì„í•˜ê¸°', 'ê°€ì¡±ê³¼ ì‹œê°„ ë³´ë‚´ê³  ì†Œí†µí•˜ê¸°',\n",
        "        'ë™ë£Œë“¤ê³¼ íšŒì‹í•˜ê³  ì¹œëª© ë„ëª¨í•˜ê¸°', 'ìƒˆë¡œìš´ ì‚¬ëŒë“¤ê³¼ ë„¤íŠ¸ì›Œí‚¹í•˜ê¸°', 'ì»¤ë®¤ë‹ˆí‹° í™œë™ ì°¸ì—¬í•˜ê¸°',\n",
        "        'ì‚¬íšŒì  ê´€ê³„ ìœ ì§€í•˜ê³  ë°œì „ì‹œí‚¤ê¸°', 'ì§€ì¸ë“¤ê³¼ ì—°ë½í•˜ê³  ì•ˆë¶€ ë¬»ê¸°'\n",
        "    ],\n",
        "    'ì·¨ë¯¸': [\n",
        "        'ë…ì„œí•˜ë©° ì§€ì‹ ìŒ“ê³  ì—¬ê°€ ì¦ê¸°ê¸°', 'ì˜í™”ë‚˜ ë“œë¼ë§ˆ ì‹œì²­í•˜ê¸°', 'ìš”ë¦¬ ë ˆì‹œí”¼ ë„ì „í•˜ê³  ìŒì‹ ë§Œë“¤ê¸°',\n",
        "        'ì—¬í–‰ ê³„íš ì„¸ìš°ê³  ë– ë‚˜ê¸°', 'ì·¨ë¯¸ í´ë˜ìŠ¤ ìˆ˜ê°•í•˜ê³  ìƒˆ ê¸°ìˆ  ë°°ìš°ê¸°', 'ê²Œì„í•˜ë©° ìŠ¤íŠ¸ë ˆìŠ¤ í’€ê¸°',\n",
        "        'ìŒì•… ë“£ê³  ì•…ê¸° ì—°ì£¼í•˜ê¸°', 'ê·¸ë¦¼ ê·¸ë¦¬ê³  ì°½ì‘ í™œë™í•˜ê¸°'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# NER ê´€ë ¨ ì„¤ì •\n",
        "NER_SPECIAL_TOKENS = [\"<PERSON>\", \"<LOCATION>\", \"<ORG>\"]\n",
        "V2_IMPROVED_ENTITIES = [\"PS\", \"LC\", \"OG\"]  # ì¸ë¬¼, ì¥ì†Œ, ì¡°ì§\n",
        "NER_CONFIDENCE_THRESHOLD = 0.8\n",
        "\n",
        "print(\"âœ… ì„¤ì • ë° ì¹´í…Œê³ ë¦¬ ì •ì˜ ì™„ë£Œ\")\n",
        "print(f\"ğŸ“ ì´ {len(CATEGORIES_DEFINITIONS)}ê°œ ì¹´í…Œê³ ë¦¬:\")\n",
        "for cat, keywords in CATEGORIES_DEFINITIONS.items():\n",
        "    print(f\"  - {cat}: {len(keywords)}ê°œ í‚¤ì›Œë“œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ§  NER ëª¨ë¸ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def setup_ner_model():\n",
        "    \"\"\"NER ëª¨ë¸ì„ ì„¤ì •í•˜ê³  ë°˜í™˜\"\"\"\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)\n",
        "        model = AutoModelForTokenClassification.from_pretrained(NER_MODEL_NAME)\n",
        "        ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0 if device == \"cuda\" else -1)\n",
        "        print(\"âœ… NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
        "        return ner_pipeline\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ NER ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        return None\n",
        "\n",
        "def ner_generalize_texts(texts, entities_to_generalize=V2_IMPROVED_ENTITIES, confidence_threshold=NER_CONFIDENCE_THRESHOLD):\n",
        "    \"\"\"NERì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¼ë°˜í™”\"\"\"\n",
        "    ner_pipeline = setup_ner_model()\n",
        "    if not ner_pipeline:\n",
        "        return texts  # NER ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜\n",
        "    \n",
        "    generalized_texts = []\n",
        "    \n",
        "    for text in tqdm(texts, desc=\"NER ì¼ë°˜í™” ì§„í–‰\"):\n",
        "        try:\n",
        "            # NER ìˆ˜í–‰\n",
        "            entities = ner_pipeline(text)\n",
        "            \n",
        "            # ì‹ ë¢°ë„ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§\n",
        "            filtered_entities = [\n",
        "                entity for entity in entities\n",
        "                if entity['score'] >= confidence_threshold and \n",
        "                entity['entity'].replace('B-', '').replace('I-', '') in entities_to_generalize\n",
        "            ]\n",
        "            \n",
        "            # ì—”í‹°í‹°ë¥¼ ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë’¤ì—ì„œë¶€í„° ì¹˜í™˜í•˜ê¸° ìœ„í•´ ì—­ìˆœ)\n",
        "            filtered_entities.sort(key=lambda x: x['start'], reverse=True)\n",
        "            \n",
        "            # í…ìŠ¤íŠ¸ ì¹˜í™˜\n",
        "            generalized_text = text\n",
        "            for entity in filtered_entities:\n",
        "                entity_type = entity['entity'].replace('B-', '').replace('I-', '')\n",
        "                if entity_type == 'PS':\n",
        "                    replacement = '<PERSON>'\n",
        "                elif entity_type == 'LC':\n",
        "                    replacement = '<LOCATION>'\n",
        "                elif entity_type == 'OG':\n",
        "                    replacement = '<ORG>'\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                generalized_text = (\n",
        "                    generalized_text[:entity['start']] + \n",
        "                    replacement + \n",
        "                    generalized_text[entity['end']:]\n",
        "                )\n",
        "            \n",
        "            generalized_texts.append(generalized_text)\n",
        "            \n",
        "        except Exception as e:\n",
        "            # ì˜¤ë¥˜ ë°œìƒì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
        "            generalized_texts.append(text)\n",
        "    \n",
        "    return generalized_texts\n",
        "\n",
        "print(\"âœ… NER ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ ì•™ìƒë¸” ìµœì í™” í´ë˜ìŠ¤\n",
        "class EnsembleOptimizer:\n",
        "    \"\"\"ì•™ìƒë¸” ê¸°ë²•ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, sample_size=None):\n",
        "        self.data_path = data_path\n",
        "        self.sample_size = sample_size  # Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
        "        \n",
        "        # ì¹´í…Œê³ ë¦¬ ì •ì˜ (ê°€ì¥ ë¨¼ì € ì„¤ì •)\n",
        "        self.categories = list(CATEGORIES_DEFINITIONS.keys())\n",
        "        \n",
        "        # ëª¨ë¸ ë¡œë“œ\n",
        "        print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
        "        self.base_model = SentenceTransformer(BASE_MODEL_NAME, device=device)\n",
        "        \n",
        "        # ë°ì´í„° ì¤€ë¹„\n",
        "        self.test_df = self._prepare_data()\n",
        "        \n",
        "        # ê¸°ë³¸ ì„ë² ë”©ë“¤ ë¯¸ë¦¬ ê³„ì‚°\n",
        "        self._precompute_embeddings()\n",
        "        \n",
        "    def _prepare_data(self):\n",
        "        \"\"\"ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\"\"\"\n",
        "        df = pd.read_csv(self.data_path)\n",
        "        \n",
        "        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì •ê·œí™”\n",
        "        if 'categories' not in df.columns and 'category' in df.columns:\n",
        "            df = df.rename(columns={'category': 'categories'})\n",
        "        \n",
        "        df.dropna(subset=['title', 'categories'], inplace=True)\n",
        "        df['category'] = df['categories'].apply(lambda x: x.split(';')[0].strip() if isinstance(x, str) else x)\n",
        "        df = df[df['category'].isin(self.categories)]\n",
        "        \n",
        "        # ìƒ˜í”Œë§ (sample_sizeê°€ Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
        "        if self.sample_size is not None and len(df) > self.sample_size:\n",
        "            df = df.sample(n=self.sample_size, random_state=42)\n",
        "            print(f\"âš¡ ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ {self.sample_size}ê°œ ìƒ˜í”Œë¡œ ì œí•œ\")\n",
        "        else:\n",
        "            print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(df)}ê°œ\")\n",
        "        \n",
        "        # NER ì „ì²˜ë¦¬\n",
        "        print(\"ğŸ”„ NER ì „ì²˜ë¦¬ ì¤‘...\")\n",
        "        df = df.copy()\n",
        "        df['generalized_title'] = ner_generalize_texts(df['title'].tolist())\n",
        "        \n",
        "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(df)}ê°œ\")\n",
        "        return df\n",
        "    \n",
        "    def _precompute_embeddings(self):\n",
        "        \"\"\"ê¸°ë³¸ ì„ë² ë”©ë“¤ì„ ë¯¸ë¦¬ ê³„ì‚°\"\"\"\n",
        "        print(\"ğŸ”„ ê¸°ë³¸ ì„ë² ë”© ê³„ì‚° ì¤‘...\")\n",
        "        \n",
        "        # ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª… ì„ë² ë”©\n",
        "        self.simple_cat_embs = self.base_model.encode(\n",
        "            self.categories, \n",
        "            convert_to_tensor=True, \n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "        \n",
        "        # í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”©\n",
        "        self.keyword_avg_embs = self._get_keyword_avg_embs()\n",
        "        \n",
        "        print(\"âœ… ê¸°ë³¸ ì„ë² ë”© ê³„ì‚° ì™„ë£Œ\")\n",
        "    \n",
        "    def _get_keyword_avg_embs(self):\n",
        "        \"\"\"ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”© ê³„ì‚°\"\"\"\n",
        "        category_embs = {}\n",
        "        for category, keywords in CATEGORIES_DEFINITIONS.items():\n",
        "            keyword_embs = self.base_model.encode(keywords, convert_to_numpy=True, normalize_embeddings=True)\n",
        "            avg_emb = np.mean(keyword_embs, axis=0)\n",
        "            if np.linalg.norm(avg_emb) > 0:\n",
        "                avg_emb = avg_emb / np.linalg.norm(avg_emb)\n",
        "            category_embs[category] = avg_emb\n",
        "        return torch.tensor(np.array(list(category_embs.values()))).to(device)\n",
        "\n",
        "print(\"âœ… ì•™ìƒë¸” ìµœì í™” í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ” ì•™ìƒë¸” í‰ê°€ ë©”ì†Œë“œë“¤ ì¶”ê°€ (ê¸°ì¡´ í´ë˜ìŠ¤ì— ë©”ì†Œë“œ ì¶”ê°€)\n",
        "def _ensemble_predict(self, ensemble_configs, text_column='generalized_title'):\n",
        "    \"\"\"ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰\"\"\"\n",
        "    all_similarities = []\n",
        "    weights = []\n",
        "    \n",
        "    for config in ensemble_configs:\n",
        "        model_type = config['model']\n",
        "        embedding_type = config['embedding']\n",
        "        weight = config['weight']\n",
        "        \n",
        "        # ëª¨ë¸ ì„ íƒ (ì—¬ê¸°ì„œëŠ” base ëª¨ë¸ë§Œ ì‚¬ìš©)\n",
        "        model = self.base_model\n",
        "        \n",
        "        # ì„ë² ë”© íƒ€ì…ì— ë”°ë¥¸ ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì„ íƒ\n",
        "        if embedding_type == 'simple':\n",
        "            category_embs = self.simple_cat_embs\n",
        "        elif embedding_type == 'keyword_avg':\n",
        "            category_embs = self.keyword_avg_embs\n",
        "        else:\n",
        "            continue\n",
        "            \n",
        "        # í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚°\n",
        "        text_embs = model.encode(\n",
        "            self.test_df[text_column].tolist(), \n",
        "            convert_to_tensor=True, \n",
        "            normalize_embeddings=True\n",
        "        )\n",
        "        \n",
        "        # ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        similarities = util.cos_sim(text_embs, category_embs).cpu().numpy()\n",
        "        all_similarities.append(similarities)\n",
        "        weights.append(weight)\n",
        "    \n",
        "    if not all_similarities:\n",
        "        return None\n",
        "    \n",
        "    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸”\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / weights.sum()  # ì •ê·œí™”\n",
        "    \n",
        "    ensemble_similarities = np.zeros_like(all_similarities[0])\n",
        "    for sim, w in zip(all_similarities, weights):\n",
        "        ensemble_similarities += sim * w\n",
        "        \n",
        "    return ensemble_similarities\n",
        "\n",
        "def _evaluate_ensemble(self, ensemble_configs, text_column='generalized_title'):\n",
        "    \"\"\"ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€\"\"\"\n",
        "    similarities = self._ensemble_predict(ensemble_configs, text_column)\n",
        "    if similarities is None:\n",
        "        return {}\n",
        "    \n",
        "    # ì˜ˆì¸¡ ê²°ê³¼ ê³„ì‚°\n",
        "    pred_indices = np.argsort(similarities, axis=1)[:, ::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
        "    \n",
        "    true_categories = self.test_df['category'].tolist()\n",
        "    true_indices = [self.categories.index(cat) for cat in true_categories]\n",
        "    \n",
        "    # Hit Rate ê³„ì‚°\n",
        "    correct_at_1 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx == pred_indices[i, 0])\n",
        "    correct_at_3 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in pred_indices[i, :3])\n",
        "    \n",
        "    total_count = len(self.test_df)\n",
        "    hit_rate_1 = correct_at_1 / total_count\n",
        "    hit_rate_3 = correct_at_3 / total_count\n",
        "    \n",
        "    # F1-score ê³„ì‚°\n",
        "    top_1_predictions = [self.categories[idx] for idx in pred_indices[:, 0]]\n",
        "    _, _, f1, _ = precision_recall_fscore_support(\n",
        "        true_categories, top_1_predictions, average='macro', zero_division=0\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'hit_rate_1': hit_rate_1,\n",
        "        'hit_rate_3': hit_rate_3,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "# í´ë˜ìŠ¤ì— ë©”ì†Œë“œ ì¶”ê°€\n",
        "EnsembleOptimizer._ensemble_predict = _ensemble_predict\n",
        "EnsembleOptimizer._evaluate_ensemble = _evaluate_ensemble\n",
        "\n",
        "print(\"âœ… ì•™ìƒë¸” í‰ê°€ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë©”ì†Œë“œ (ê¸°ì¡´ í´ë˜ìŠ¤ì— ë©”ì†Œë“œ ì¶”ê°€)\n",
        "def optimize_hyperparameters(self):\n",
        "    \"\"\"ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\"\"\"\n",
        "    print(\"ğŸš€ ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘\")\n",
        "    \n",
        "    # ìµœì í™”í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (ì „ì²´ ë°ì´í„° í™œìš©ìœ¼ë¡œ í™•ì¥)\n",
        "    ensemble_sizes = [2, 3]  # ì•™ìƒë¸” êµ¬ì„±ìš”ì†Œ ê°œìˆ˜\n",
        "    weight_combinations = [\n",
        "        # 2-way ì•™ìƒë¸”\n",
        "        [0.5, 0.5],\n",
        "        [0.6, 0.4],\n",
        "        [0.7, 0.3],\n",
        "        [0.8, 0.2],\n",
        "        [0.4, 0.6],\n",
        "        [0.3, 0.7],\n",
        "        # 3-way ì•™ìƒë¸” (í–¥í›„ í™•ì¥ìš©)\n",
        "        [0.4, 0.3, 0.3],\n",
        "        [0.5, 0.3, 0.2],\n",
        "        [0.6, 0.3, 0.1],\n",
        "        [0.33, 0.33, 0.34]\n",
        "    ]\n",
        "    \n",
        "    # ê¸°ë³¸ êµ¬ì„±ìš”ì†Œë“¤\n",
        "    base_components = [\n",
        "        {'model': 'base', 'embedding': 'simple'},\n",
        "        {'model': 'base', 'embedding': 'keyword_avg'}\n",
        "    ]\n",
        "    \n",
        "    best_results = {}\n",
        "    best_configs = {}\n",
        "    \n",
        "    print(f\"ğŸ“Š ì´ {len(weight_combinations)}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸\")\n",
        "    \n",
        "    for i, weights in enumerate(weight_combinations):\n",
        "        ensemble_size = len(weights)\n",
        "        if ensemble_size > len(base_components):\n",
        "            continue\n",
        "            \n",
        "        # ì•™ìƒë¸” êµ¬ì„± ìƒì„±\n",
        "        ensemble_config = []\n",
        "        for j in range(ensemble_size):\n",
        "            config = base_components[j].copy()\n",
        "            config['weight'] = weights[j]\n",
        "            ensemble_config.append(config)\n",
        "        \n",
        "        # í‰ê°€ ìˆ˜í–‰\n",
        "        print(f\"\\\\nğŸ”„ ì¡°í•© {i+1}/{len(weight_combinations)} í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
        "        print(f\"   êµ¬ì„±: {ensemble_config}\")\n",
        "        \n",
        "        results = self._evaluate_ensemble(ensemble_config)\n",
        "        if not results:\n",
        "            continue\n",
        "        \n",
        "        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸\n",
        "        for metric in ['hit_rate_1', 'hit_rate_3', 'f1_score']:\n",
        "            if metric not in best_results or results[metric] > best_results[metric]:\n",
        "                best_results[metric] = results[metric]\n",
        "                best_configs[metric] = {\n",
        "                    'config': ensemble_config,\n",
        "                    'results': results\n",
        "                }\n",
        "        \n",
        "        print(f\"   ê²°ê³¼: Hit@1={results['hit_rate_1']:.3f}, Hit@3={results['hit_rate_3']:.3f}, F1={results['f1_score']:.3f}\")\n",
        "    \n",
        "    return best_configs, best_results\n",
        "\n",
        "def run_baseline_comparison(self):\n",
        "    \"\"\"ê¸°ë³¸ ë²„ì „ë“¤ê³¼ ì„±ëŠ¥ ë¹„êµ\"\"\"\n",
        "    print(\"\\\\nğŸ“Š ê¸°ë³¸ ë²„ì „ë“¤ ì„±ëŠ¥ ì¸¡ì •\")\n",
        "    \n",
        "    baselines = {}\n",
        "    \n",
        "    # V1: ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª… ë§¤ì¹­\n",
        "    print(\"ğŸ”„ V1 (ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª…) í‰ê°€ ì¤‘...\")\n",
        "    baselines['V1'] = self._evaluate_single_method('simple', 'title')\n",
        "    \n",
        "    # V2: NER + ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª…\n",
        "    print(\"ğŸ”„ V2 (NER + ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª…) í‰ê°€ ì¤‘...\")\n",
        "    baselines['V2'] = self._evaluate_single_method('simple', 'generalized_title')\n",
        "    \n",
        "    # V3: NER + í‚¤ì›Œë“œ í‰ê· \n",
        "    print(\"ğŸ”„ V3 (NER + í‚¤ì›Œë“œ í‰ê· ) í‰ê°€ ì¤‘...\")\n",
        "    baselines['V3'] = self._evaluate_single_method('keyword_avg', 'generalized_title')\n",
        "    \n",
        "    return baselines\n",
        "\n",
        "def _evaluate_single_method(self, embedding_type, text_column):\n",
        "    \"\"\"ë‹¨ì¼ ë°©ë²• í‰ê°€\"\"\"\n",
        "    config = [{'model': 'base', 'embedding': embedding_type, 'weight': 1.0}]\n",
        "    return self._evaluate_ensemble(config, text_column)\n",
        "\n",
        "# í´ë˜ìŠ¤ì— ë©”ì†Œë“œ ì¶”ê°€\n",
        "EnsembleOptimizer.optimize_hyperparameters = optimize_hyperparameters\n",
        "EnsembleOptimizer.run_baseline_comparison = run_baseline_comparison\n",
        "EnsembleOptimizer._evaluate_single_method = _evaluate_single_method\n",
        "\n",
        "print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š ê²°ê³¼ ì‹œê°í™” ë° ì¶œë ¥ í•¨ìˆ˜\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_results(baselines, best_ensemble_results, title=\"ì„±ëŠ¥ ë¹„êµ\"):\n",
        "    \"\"\"ê²°ê³¼ë¥¼ ì‹œê°í™”\"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    # ë°ì´í„° ì¤€ë¹„\n",
        "    methods = list(baselines.keys()) + ['Ensemble (Best)']\n",
        "    hit_rate_1_scores = [baselines[method]['hit_rate_1'] for method in baselines.keys()]\n",
        "    hit_rate_3_scores = [baselines[method]['hit_rate_3'] for method in baselines.keys()]\n",
        "    f1_scores = [baselines[method]['f1_score'] for method in baselines.keys()]\n",
        "    \n",
        "    # ìµœê³  ì•™ìƒë¸” ì„±ëŠ¥ ì¶”ê°€\n",
        "    if 'hit_rate_1' in best_ensemble_results:\n",
        "        hit_rate_1_scores.append(best_ensemble_results['hit_rate_1'])\n",
        "        hit_rate_3_scores.append(best_ensemble_results['hit_rate_3'])\n",
        "        f1_scores.append(best_ensemble_results['f1_score'])\n",
        "    \n",
        "    # ì„œë¸Œí”Œë¡¯ ìƒì„±\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Hit Rate @1\n",
        "    axes[0].bar(methods, hit_rate_1_scores, color='skyblue', alpha=0.8)\n",
        "    axes[0].set_title('Hit Rate @1')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Hit Rate @3\n",
        "    axes[1].bar(methods, hit_rate_3_scores, color='lightcoral', alpha=0.8)\n",
        "    axes[1].set_title('Hit Rate @3')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # F1 Score\n",
        "    axes[2].bar(methods, f1_scores, color='lightgreen', alpha=0.8)\n",
        "    axes[2].set_title('F1 Score (Macro)')\n",
        "    axes[2].set_ylabel('F1 Score')\n",
        "    axes[2].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(title, y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "def print_detailed_results(baselines, best_configs, best_results):\n",
        "    \"\"\"ìƒì„¸ ê²°ê³¼ ì¶œë ¥\"\"\"\n",
        "    print(\"\\\\n\" + \"=\"*80)\n",
        "    print(\" \" * 25 + \"ğŸ† ìµœì¢… ì„±ëŠ¥ ë¹„êµ ê²°ê³¼\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # ê¸°ë³¸ ë²„ì „ë“¤ ê²°ê³¼\n",
        "    print(\"\\\\nğŸ“ˆ ê¸°ë³¸ ë²„ì „ë“¤ ì„±ëŠ¥:\")\n",
        "    for method, results in baselines.items():\n",
        "        print(f\"  {method:15} | Hit@1: {results['hit_rate_1']:.3f} | Hit@3: {results['hit_rate_3']:.3f} | F1: {results['f1_score']:.3f}\")\n",
        "    \n",
        "    # ìµœê³  ì•™ìƒë¸” ê²°ê³¼\n",
        "    print(\"\\\\nğŸ”¥ ìµœê³  ì•™ìƒë¸” ì„±ëŠ¥:\")\n",
        "    for metric, value in best_results.items():\n",
        "        print(f\"  {metric:15} | {value:.3f}\")\n",
        "    \n",
        "    # ìµœì  ì„¤ì • ì¶œë ¥\n",
        "    print(\"\\\\nâš™ï¸ ìµœì  ì•™ìƒë¸” ì„¤ì •:\")\n",
        "    for metric, config_info in best_configs.items():\n",
        "        print(f\"\\\\nğŸ“Š {metric} ìµœì  ì„¤ì •:\")\n",
        "        print(f\"   êµ¬ì„±: {config_info['config']}\")\n",
        "        print(f\"   ì„±ëŠ¥: {config_info['results']}\")\n",
        "\n",
        "print(\"âœ… ì‹œê°í™” ë° ì¶œë ¥ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ V3 ì•™ìƒë¸” ìµœì í™” ì‹¤í–‰\n",
        "print(\"=\"*80)\n",
        "print(\" \" * 20 + \"ğŸ”¬ V3 ì•™ìƒë¸” ê¸°ë²• ìµœì í™” ì‹¤í—˜\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ì•™ìƒë¸” ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
        "optimizer = EnsembleOptimizer(DATA_PATH, sample_size=None)  # ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
        "\n",
        "# ê¸°ë³¸ ë²„ì „ë“¤ ì„±ëŠ¥ ì¸¡ì •\n",
        "print(\"\\\\n1ï¸âƒ£ ê¸°ë³¸ ë²„ì „ë“¤(V1, V2, V3) ì„±ëŠ¥ ì¸¡ì •\")\n",
        "baselines = optimizer.run_baseline_comparison()\n",
        "\n",
        "# ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
        "print(\"\\\\n\\\\n2ï¸âƒ£ ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\")\n",
        "best_configs, best_results = optimizer.optimize_hyperparameters()\n",
        "\n",
        "# ê²°ê³¼ ì‹œê°í™”\n",
        "print(\"\\\\n\\\\n3ï¸âƒ£ ê²°ê³¼ ì‹œê°í™”\")\n",
        "plot_results(baselines, best_results, \"V3 vs ì•™ìƒë¸” ì„±ëŠ¥ ë¹„êµ\")\n",
        "\n",
        "# ìƒì„¸ ê²°ê³¼ ì¶œë ¥\n",
        "print_detailed_results(baselines, best_configs, best_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ”§ V4: íŒŒì¸íŠœë‹ + ì•™ìƒë¸” ê¸°ë²•\n",
        "\n",
        "V3ì˜ ì•™ìƒë¸” ê²°ê³¼ê°€ ì¢‹ë‹¤ë©´, ì´ì œ íŒŒì¸íŠœë‹ì„ ì¶”ê°€ë¡œ ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ ì •ì˜\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class QuickFinetuner:\n",
        "    \"\"\"í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ íŒŒì¸íŠœë‹ í´ë˜ìŠ¤\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, sample_size=500):\n",
        "        self.data_path = data_path\n",
        "        self.sample_size = sample_size  # ê¸°ë³¸ê°’ì„ 500ìœ¼ë¡œ ì¦ê°€\n",
        "        self.categories_definitions = CATEGORIES_DEFINITIONS\n",
        "        self.categories = list(CATEGORIES_DEFINITIONS.keys())  # categories ì†ì„± ì¶”ê°€\n",
        "        \n",
        "        # ë°ì´í„° ì¤€ë¹„\n",
        "        self.train_df, self.test_df = self._prepare_data()\n",
        "        \n",
        "    def _prepare_data(self):\n",
        "        \"\"\"íŒŒì¸íŠœë‹ìš© ë°ì´í„° ì¤€ë¹„\"\"\"\n",
        "        df = pd.read_csv(self.data_path)\n",
        "        \n",
        "        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì •ê·œí™”\n",
        "        if 'categories' not in df.columns and 'category' in df.columns:\n",
        "            df = df.rename(columns={'category': 'categories'})\n",
        "        \n",
        "        df.dropna(subset=['title', 'categories'], inplace=True)\n",
        "        df['category'] = df['categories'].apply(lambda x: x.split(';')[0].strip() if isinstance(x, str) else x)\n",
        "        df = df[df['category'].isin(list(CATEGORIES_DEFINITIONS.keys()))]\n",
        "        \n",
        "        # íŒŒì¸íŠœë‹ìš© ë°ì´í„° ìƒ˜í”Œë§\n",
        "        if len(df) > self.sample_size:\n",
        "            df = df.sample(n=self.sample_size, random_state=42)\n",
        "            print(f\"ğŸ“Š íŒŒì¸íŠœë‹ìš© ë°ì´í„°: {self.sample_size}ê°œ (ì „ì²´ {len(df)}ê°œì—ì„œ ìƒ˜í”Œë§)\")\n",
        "        else:\n",
        "            print(f\"ğŸ“Š ì „ì²´ ë°ì´í„° ì‚¬ìš©: {len(df)}ê°œ\")\n",
        "        \n",
        "        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
        "        train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "        \n",
        "        # NER ì „ì²˜ë¦¬\n",
        "        train_df = train_df.copy()\n",
        "        test_df = test_df.copy()\n",
        "        train_df['generalized_title'] = ner_generalize_texts(train_df['title'].tolist())\n",
        "        test_df['generalized_title'] = ner_generalize_texts(test_df['title'].tolist())\n",
        "        \n",
        "        print(f\"âœ… íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„: í›ˆë ¨ {len(train_df)}ê°œ, í…ŒìŠ¤íŠ¸ {len(test_df)}ê°œ\")\n",
        "        return train_df, test_df\n",
        "    \n",
        "    def create_training_examples(self):\n",
        "        \"\"\"íŒŒì¸íŠœë‹ìš© InputExample ìƒì„±\"\"\"\n",
        "        examples = []\n",
        "        for _, row in self.train_df.iterrows():\n",
        "            title = row['generalized_title']\n",
        "            category = row['category']\n",
        "            \n",
        "            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë“¤ê³¼ positive example ìƒì„±\n",
        "            if category in self.categories_definitions:\n",
        "                for keyword in self.categories_definitions[category]:\n",
        "                    examples.append(InputExample(texts=[title, keyword]))\n",
        "        \n",
        "        print(f\"âœ… íŒŒì¸íŠœë‹ ì˜ˆì‹œ ìƒì„±: {len(examples)}ê°œ\")\n",
        "        return examples\n",
        "\n",
        "print(\"âœ… íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ V4 íŒŒì¸íŠœë‹ ì‹¤í–‰ (ì¡°ê±´ë¶€)\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\" \" * 15 + \"ğŸ“ V4: íŒŒì¸íŠœë‹ + ì•™ìƒë¸” ê¸°ë²• ì ìš©\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# V3 ì•™ìƒë¸” ê²°ê³¼ê°€ ê¸°ë³¸ V3ë³´ë‹¤ ì¢‹ì€ì§€ í™•ì¸\n",
        "v3_baseline = baselines['V3']['hit_rate_1']\n",
        "best_ensemble_hit1 = best_results.get('hit_rate_1', 0)\n",
        "\n",
        "print(f\"\\\\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:\")\n",
        "print(f\"  V3 ê¸°ë³¸: {v3_baseline:.3f}\")\n",
        "print(f\"  V3 ì•™ìƒë¸”: {best_ensemble_hit1:.3f}\")\n",
        "print(f\"  í–¥ìƒë„: {((best_ensemble_hit1 - v3_baseline) / v3_baseline * 100):.1f}%\")\n",
        "\n",
        "# ì•™ìƒë¸”ì´ í–¥ìƒë˜ì—ˆë‹¤ë©´ íŒŒì¸íŠœë‹ë„ ì ìš©\n",
        "if best_ensemble_hit1 > v3_baseline:\n",
        "    print(\"\\\\nâœ… ì•™ìƒë¸” ê¸°ë²•ì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤!\")\n",
        "    print(\"ğŸ”„ V4 íŒŒì¸íŠœë‹ì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
        "    \n",
        "    # íŒŒì¸íŠœë‹ ì‹¤í–‰ (ë” ë§ì€ ë°ì´í„° ì‚¬ìš©)\n",
        "    finetuner = QuickFinetuner(DATA_PATH, sample_size=800)  # ì „ì²´ ë°ì´í„°ì˜ ì ˆë°˜ ì •ë„ ì‚¬ìš©\n",
        "    \n",
        "    # íŒŒì¸íŠœë‹ (ë” ë§ì€ ì—í¬í¬ë¡œ í–¥ìƒëœ í•™ìŠµ)\n",
        "    print(\"\\\\nğŸ“ íŒŒì¸íŠœë‹ ì‹¤í–‰ ì¤‘... (3 epochs)\")\n",
        "    finetuned_model = finetuner.quick_finetune(epochs=3)\n",
        "    \n",
        "    print(\"âœ… V4 íŒŒì¸íŠœë‹ ì™„ë£Œ!\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\\\nâŒ ì•™ìƒë¸” ê¸°ë²•ì´ í° í–¥ìƒì„ ë³´ì´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"íŒŒì¸íŠœë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "    finetuned_model = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”„ íŒŒì¸íŠœë‹ ë©”ì†Œë“œ ì¶”ê°€\n",
        "class QuickFinetuner(QuickFinetuner):  # í´ë˜ìŠ¤ í™•ì¥\n",
        "    \n",
        "    def quick_finetune(self, epochs=1):\n",
        "        \"\"\"ë¹ ë¥¸ íŒŒì¸íŠœë‹ ìˆ˜í–‰\"\"\"\n",
        "        print(\"ğŸ”„ íŒŒì¸íŠœë‹ ì‹œì‘...\")\n",
        "        \n",
        "        # wandb ë¹„í™œì„±í™” í™•ì¸\n",
        "        os.environ['WANDB_DISABLED'] = 'true'\n",
        "        \n",
        "        # ëª¨ë¸ ë¡œë“œ\n",
        "        model = SentenceTransformer(BASE_MODEL_NAME, device=device)\n",
        "        \n",
        "        # íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
        "        model.tokenizer.add_special_tokens({\"additional_special_tokens\": NER_SPECIAL_TOKENS})\n",
        "        model._first_module().auto_model.resize_token_embeddings(len(model.tokenizer))\n",
        "        \n",
        "        # í›ˆë ¨ ì˜ˆì‹œ ìƒì„±\n",
        "        train_examples = self.create_training_examples()\n",
        "        \n",
        "        # ë°ì´í„°ë¡œë” ìƒì„±\n",
        "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
        "        \n",
        "        # ì†ì‹¤ í•¨ìˆ˜\n",
        "        train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "        \n",
        "        # í›ˆë ¨ ì‹¤í–‰\n",
        "        model.fit(\n",
        "            train_objectives=[(train_dataloader, train_loss)],\n",
        "            epochs=epochs,\n",
        "            warmup_steps=max(1, int(len(train_dataloader) * 0.1)),\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ\")\n",
        "        return model\n",
        "\n",
        "print(\"âœ… íŒŒì¸íŠœë‹ ë©”ì†Œë“œ ì¶”ê°€ ì™„ë£Œ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¯ V4 ì•™ìƒë¸” í‰ê°€ (íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í¬í•¨)\n",
        "if finetuned_model is not None:\n",
        "    print(\"\\\\n\" + \"=\"*60)\n",
        "    print(\" \" * 15 + \"ğŸ”¥ V4 ì•™ìƒë¸” í‰ê°€\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ V4 ì•™ìƒë¸” í‰ê°€\n",
        "    class V4EnsembleEvaluator:\n",
        "        \"\"\"V4ìš© ì•™ìƒë¸” í‰ê°€ê¸°\"\"\"\n",
        "        \n",
        "        def __init__(self, test_df, base_model, finetuned_model):\n",
        "            self.test_df = test_df\n",
        "            self.base_model = base_model\n",
        "            self.finetuned_model = finetuned_model\n",
        "            self.categories = list(CATEGORIES_DEFINITIONS.keys())\n",
        "            \n",
        "            # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ê³„ì‚°\n",
        "            self.finetuned_keyword_embs = self._get_keyword_avg_embs(finetuned_model)\n",
        "            self.base_keyword_embs = self._get_keyword_avg_embs(base_model)\n",
        "            \n",
        "        def _get_keyword_avg_embs(self, model):\n",
        "            \"\"\"í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”© ê³„ì‚°\"\"\"\n",
        "            category_embs = {}\n",
        "            for category, keywords in CATEGORIES_DEFINITIONS.items():\n",
        "                keyword_embs = model.encode(keywords, convert_to_numpy=True, normalize_embeddings=True)\n",
        "                avg_emb = np.mean(keyword_embs, axis=0)\n",
        "                if np.linalg.norm(avg_emb) > 0:\n",
        "                    avg_emb = avg_emb / np.linalg.norm(avg_emb)\n",
        "                category_embs[category] = avg_emb\n",
        "            return torch.tensor(np.array(list(category_embs.values()))).to(device)\n",
        "        \n",
        "        def evaluate_v4_ensemble(self, ensemble_config):\n",
        "            \"\"\"V4 ì•™ìƒë¸” í‰ê°€\"\"\"\n",
        "            all_similarities = []\n",
        "            weights = []\n",
        "            \n",
        "            for config in ensemble_config:\n",
        "                model_type = config['model']\n",
        "                embedding_type = config['embedding']\n",
        "                weight = config['weight']\n",
        "                \n",
        "                # ëª¨ë¸ ì„ íƒ\n",
        "                if model_type == 'base':\n",
        "                    model = self.base_model\n",
        "                    category_embs = self.base_keyword_embs\n",
        "                elif model_type == 'finetuned':\n",
        "                    model = self.finetuned_model\n",
        "                    category_embs = self.finetuned_keyword_embs\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                # í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
        "                text_embs = model.encode(\n",
        "                    self.test_df['generalized_title'].tolist(),\n",
        "                    convert_to_tensor=True,\n",
        "                    normalize_embeddings=True\n",
        "                )\n",
        "                \n",
        "                # ìœ ì‚¬ë„ ê³„ì‚°\n",
        "                similarities = util.cos_sim(text_embs, category_embs).cpu().numpy()\n",
        "                all_similarities.append(similarities)\n",
        "                weights.append(weight)\n",
        "            \n",
        "            # ì•™ìƒë¸” ê³„ì‚°\n",
        "            if not all_similarities:\n",
        "                return {}\n",
        "            \n",
        "            weights = np.array(weights) / np.sum(weights)\n",
        "            ensemble_similarities = np.zeros_like(all_similarities[0])\n",
        "            for sim, w in zip(all_similarities, weights):\n",
        "                ensemble_similarities += sim * w\n",
        "            \n",
        "            # ì„±ëŠ¥ ê³„ì‚°\n",
        "            pred_indices = np.argsort(ensemble_similarities, axis=1)[:, ::-1]\n",
        "            true_categories = self.test_df['category'].tolist()\n",
        "            true_indices = [self.categories.index(cat) for cat in true_categories]\n",
        "            \n",
        "            correct_at_1 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx == pred_indices[i, 0])\n",
        "            correct_at_3 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in pred_indices[i, :3])\n",
        "            \n",
        "            total_count = len(self.test_df)\n",
        "            hit_rate_1 = correct_at_1 / total_count\n",
        "            hit_rate_3 = correct_at_3 / total_count\n",
        "            \n",
        "            # F1 ê³„ì‚°\n",
        "            top_1_predictions = [self.categories[idx] for idx in pred_indices[:, 0]]\n",
        "            _, _, f1, _ = precision_recall_fscore_support(\n",
        "                true_categories, top_1_predictions, average='macro', zero_division=0\n",
        "            )\n",
        "            \n",
        "            return {\n",
        "                'hit_rate_1': hit_rate_1,\n",
        "                'hit_rate_3': hit_rate_3,\n",
        "                'f1_score': f1\n",
        "            }\n",
        "    \n",
        "    # V4 í‰ê°€ ì‹¤í–‰\n",
        "    v4_evaluator = V4EnsembleEvaluator(optimizer.test_df, optimizer.base_model, finetuned_model)\n",
        "    \n",
        "    # ì—¬ëŸ¬ V4 ì•™ìƒë¸” êµ¬ì„± í…ŒìŠ¤íŠ¸\n",
        "    v4_configs = [\n",
        "        # Base + Finetuned ì¡°í•©\n",
        "        [\n",
        "            {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.4},\n",
        "            {'model': 'finetuned', 'embedding': 'keyword_avg', 'weight': 0.6}\n",
        "        ],\n",
        "        [\n",
        "            {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.3},\n",
        "            {'model': 'finetuned', 'embedding': 'keyword_avg', 'weight': 0.7}\n",
        "        ],\n",
        "        [\n",
        "            {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.5},\n",
        "            {'model': 'finetuned', 'embedding': 'keyword_avg', 'weight': 0.5}\n",
        "        ]\n",
        "    ]\n",
        "    \n",
        "    print(\"\\\\nğŸ” V4 ì•™ìƒë¸” êµ¬ì„± í…ŒìŠ¤íŠ¸:\")\n",
        "    v4_results = {}\n",
        "    \n",
        "    for i, config in enumerate(v4_configs):\n",
        "        result = v4_evaluator.evaluate_v4_ensemble(config)\n",
        "        v4_results[f'V4_Config_{i+1}'] = result\n",
        "        print(f\"\\\\nêµ¬ì„± {i+1}: {config}\")\n",
        "        print(f\"ê²°ê³¼: Hit@1={result['hit_rate_1']:.3f}, Hit@3={result['hit_rate_3']:.3f}, F1={result['f1_score']:.3f}\")\n",
        "    \n",
        "    # ìµœê³  V4 ê²°ê³¼ ì°¾ê¸°\n",
        "    best_v4 = max(v4_results.items(), key=lambda x: x[1]['hit_rate_1'])\n",
        "    print(f\"\\\\nğŸ† ìµœê³  V4 ì„±ëŠ¥: {best_v4[0]} - Hit@1: {best_v4[1]['hit_rate_1']:.3f}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\\\nâ­ï¸ íŒŒì¸íŠœë‹ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•„ V4 í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ˆ ìµœì¢… ê²°ê³¼ ì¢…í•© ë° ì‹œê°í™”\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\" \" * 20 + \"ğŸ† ìµœì¢… ì¢…í•© ê²°ê³¼\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ëª¨ë“  ê²°ê³¼ í†µí•©\n",
        "all_results = {\n",
        "    'V1 (ê¸°ë³¸)': baselines['V1'],\n",
        "    'V2 (NER)': baselines['V2'], \n",
        "    'V3 (NER+í‚¤ì›Œë“œ)': baselines['V3'],\n",
        "    'V3 ì•™ìƒë¸”': {\n",
        "        'hit_rate_1': best_results.get('hit_rate_1', 0),\n",
        "        'hit_rate_3': best_results.get('hit_rate_3', 0),\n",
        "        'f1_score': best_results.get('f1_score', 0)\n",
        "    }\n",
        "}\n",
        "\n",
        "# V4 ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¶”ê°€\n",
        "if finetuned_model is not None and 'best_v4' in locals():\n",
        "    all_results['V4 (íŒŒì¸íŠœë‹+ì•™ìƒë¸”)'] = best_v4[1]\n",
        "\n",
        "# ìµœì¢… ì„±ëŠ¥ í‘œ ì¶œë ¥\n",
        "print(\"\\\\nğŸ“Š ì„±ëŠ¥ ë¹„êµí‘œ:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'ë°©ë²•':<20} {'Hit@1':<10} {'Hit@3':<10} {'F1-Score':<10} {'í–¥ìƒë„':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "baseline_hit1 = baselines['V1']['hit_rate_1']  # V1ì„ ê¸°ì¤€ìœ¼ë¡œ í–¥ìƒë„ ê³„ì‚°\n",
        "\n",
        "for method, results in all_results.items():\n",
        "    hit1 = results['hit_rate_1']\n",
        "    hit3 = results['hit_rate_3'] \n",
        "    f1 = results['f1_score']\n",
        "    improvement = ((hit1 - baseline_hit1) / baseline_hit1 * 100) if baseline_hit1 > 0 else 0\n",
        "    \n",
        "    print(f\"{method:<20} {hit1:<10.3f} {hit3:<10.3f} {f1:<10.3f} {improvement:<10.1f}%\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# ìµœì¢… ì‹œê°í™”\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# ì„±ëŠ¥ ì§€í‘œë³„ ì„œë¸Œí”Œë¡¯\n",
        "methods = list(all_results.keys())\n",
        "hit1_scores = [all_results[m]['hit_rate_1'] for m in methods]\n",
        "hit3_scores = [all_results[m]['hit_rate_3'] for m in methods]\n",
        "f1_scores = [all_results[m]['f1_score'] for m in methods]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Hit Rate @1\n",
        "axes[0,0].bar(methods, hit1_scores, color='skyblue', alpha=0.8)\n",
        "axes[0,0].set_title('Hit Rate @1 ë¹„êµ', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_ylabel('ì •í™•ë„')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "axes[0,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Hit Rate @3\n",
        "axes[0,1].bar(methods, hit3_scores, color='lightcoral', alpha=0.8)\n",
        "axes[0,1].set_title('Hit Rate @3 ë¹„êµ', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_ylabel('ì •í™•ë„')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "axes[0,1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# F1 Score\n",
        "axes[1,0].bar(methods, f1_scores, color='lightgreen', alpha=0.8)\n",
        "axes[1,0].set_title('F1 Score ë¹„êµ', fontsize=14, fontweight='bold')\n",
        "axes[1,0].set_ylabel('F1 Score')\n",
        "axes[1,0].tick_params(axis='x', rotation=45)\n",
        "axes[1,0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# ì¢…í•© ì„±ëŠ¥ (Hit@1 ê¸°ì¤€)\n",
        "improvements = [((score - baseline_hit1) / baseline_hit1 * 100) if baseline_hit1 > 0 else 0 for score in hit1_scores]\n",
        "colors = ['red' if imp < 0 else 'green' for imp in improvements]\n",
        "axes[1,1].bar(methods, improvements, color=colors, alpha=0.7)\n",
        "axes[1,1].set_title('V1 ëŒ€ë¹„ í–¥ìƒë„ (%)', fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_ylabel('í–¥ìƒë„ (%)')\n",
        "axes[1,1].tick_params(axis='x', rotation=45)\n",
        "axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "axes[1,1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì„±ëŠ¥ ì¢…í•© ë¹„êµ', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## ğŸ” ê²°ë¡  ë° ì¸ì‚¬ì´íŠ¸\n",
        "\n",
        "### ğŸ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­\n",
        "\n",
        "1. **ì•™ìƒë¸” ê¸°ë²•ì˜ íš¨ê³¼**\n",
        "   - ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ì•™ìƒë¸”ì´ ì¼ë°˜ì ìœ¼ë¡œ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì„\n",
        "   - ê°€ì¤‘ì¹˜ ì¡°í•©ì´ ì„±ëŠ¥ì— ì¤‘ìš”í•œ ì˜í–¥ì„ ë¯¸ì¹¨\n",
        "\n",
        "2. **NER ì „ì²˜ë¦¬ì˜ ì˜í–¥**\n",
        "   - ê°œì²´ëª… ì¸ì‹ì„ í†µí•œ í…ìŠ¤íŠ¸ ì¼ë°˜í™”ê°€ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬\n",
        "   - íŠ¹íˆ ì¸ë¬¼ëª…, ì¥ì†Œëª…ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ì—ì„œ íš¨ê³¼ì \n",
        "\n",
        "3. **í‚¤ì›Œë“œ ê¸°ë°˜ ì„ë² ë”©**\n",
        "   - ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª…ë³´ë‹¤ í–‰ë™ ì§€í–¥ì  í‚¤ì›Œë“œê°€ ë” íš¨ê³¼ì \n",
        "   - ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ ì„¤ê³„ì˜ ì¤‘ìš”ì„±\n",
        "\n",
        "4. **íŒŒì¸íŠœë‹ íš¨ê³¼**\n",
        "   - ì‘ì€ ë°ì´í„°ì…‹ì—ì„œë„ íŒŒì¸íŠœë‹ì´ ì„±ëŠ¥ ê°œì„ ì— ë„ì›€\n",
        "   - ì•™ìƒë¸”ê³¼ ê²°í•© ì‹œ ìƒìŠ¹íš¨ê³¼ ê¸°ëŒ€\n",
        "\n",
        "### ğŸ’¡ ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­\n",
        "\n",
        "1. **ë‹¨ê³„ì  ì ‘ê·¼**: V1 â†’ V2 â†’ V3 â†’ V4 ìˆœìœ¼ë¡œ ì ì§„ì  ê°œì„ \n",
        "2. **ë°ì´í„° í’ˆì§ˆ**: ê³ í’ˆì§ˆ ë¼ë²¨ë§ê³¼ í‚¤ì›Œë“œ ì„¤ê³„ê°€ í•µì‹¬\n",
        "3. **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ë„ë©”ì¸ë³„ ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰ í•„ìš”\n",
        "4. **ë¦¬ì†ŒìŠ¤ ê³ ë ¤**: ì„±ëŠ¥ ëŒ€ë¹„ ê³„ì‚° ë¹„ìš© íŠ¸ë ˆì´ë“œì˜¤í”„ ê²€í† \n",
        "\n",
        "### ğŸš€ í–¥í›„ ê°œì„  ë°©í–¥\n",
        "\n",
        "- ë” ë‹¤ì–‘í•œ ì•™ìƒë¸” ì „ëµ (stacking, voting ë“±)\n",
        "- ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ ê²€ì¦\n",
        "- ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”\n",
        "- ë„ë©”ì¸ ì ì‘ ê¸°ë²• ì ìš©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“ íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ ì •ì˜\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers.evaluation import SentenceEvaluator\n",
        "import csv\n",
        "\n",
        "class QuickFinetuner:\n",
        "    \"\"\"ë¹ ë¥¸ íŒŒì¸íŠœë‹ì„ ìœ„í•œ í´ë˜ìŠ¤ (Colab ìµœì í™”)\"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, sample_size=50):\n",
        "        self.data_path = data_path\n",
        "        self.sample_size = sample_size\n",
        "        self.categories_definitions = CATEGORIES_DEFINITIONS\n",
        "        \n",
        "        # ë°ì´í„° ì¤€ë¹„\n",
        "        self.train_df, self.test_df = self._prepare_data()\n",
        "        \n",
        "    def _prepare_data(self):\n",
        "        \"\"\"íŒŒì¸íŠœë‹ìš© ë°ì´í„° ì¤€ë¹„\"\"\"\n",
        "        df = pd.read_csv(self.data_path)\n",
        "        \n",
        "        # ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ì •ê·œí™”\n",
        "        if 'categories' not in df.columns and 'category' in df.columns:\n",
        "            df = df.rename(columns={'category': 'categories'})\n",
        "        \n",
        "        df.dropna(subset=['title', 'categories'], inplace=True)\n",
        "        df['category'] = df['categories'].apply(lambda x: x.split(';')[0].strip() if isinstance(x, str) else x)\n",
        "        df = df[df['category'].isin(list(CATEGORIES_DEFINITIONS.keys()))]\n",
        "        \n",
        "        # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ ì‘ì€ ìƒ˜í”Œ ì‚¬ìš©\n",
        "        if len(df) > self.sample_size:\n",
        "            df = df.sample(n=self.sample_size, random_state=42)\n",
        "        \n",
        "        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
        "        train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "        \n",
        "        # NER ì „ì²˜ë¦¬\n",
        "        train_df = train_df.copy()\n",
        "        test_df = test_df.copy()\n",
        "        train_df['generalized_title'] = ner_generalize_texts(train_df['title'].tolist())\n",
        "        test_df['generalized_title'] = ner_generalize_texts(test_df['title'].tolist())\n",
        "        \n",
        "        print(f\"âœ… íŒŒì¸íŠœë‹ ë°ì´í„° ì¤€ë¹„: í›ˆë ¨ {len(train_df)}ê°œ, í…ŒìŠ¤íŠ¸ {len(test_df)}ê°œ\")\n",
        "        return train_df, test_df\n",
        "    \n",
        "    def create_training_examples(self):\n",
        "        \"\"\"íŒŒì¸íŠœë‹ìš© InputExample ìƒì„±\"\"\"\n",
        "        examples = []\n",
        "        for _, row in self.train_df.iterrows():\n",
        "            title = row['generalized_title']\n",
        "            category = row['category']\n",
        "            \n",
        "            # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë“¤ê³¼ positive example ìƒì„±\n",
        "            if category in self.categories_definitions:\n",
        "                for keyword in self.categories_definitions[category]:\n",
        "                    examples.append(InputExample(texts=[title, keyword]))\n",
        "        \n",
        "        print(f\"âœ… íŒŒì¸íŠœë‹ ì˜ˆì‹œ ìƒì„±: {len(examples)}ê°œ\")\n",
        "        return examples\n",
        "    \n",
        "    def quick_finetune(self, epochs=1):\n",
        "        \"\"\"ë¹ ë¥¸ íŒŒì¸íŠœë‹ ìˆ˜í–‰\"\"\"\n",
        "        print(\"ğŸ”„ íŒŒì¸íŠœë‹ ì‹œì‘...\")\n",
        "        \n",
        "        # ëª¨ë¸ ë¡œë“œ\n",
        "        model = SentenceTransformer(BASE_MODEL_NAME, device=device)\n",
        "        \n",
        "        # í›ˆë ¨ ì˜ˆì‹œ ìƒì„±\n",
        "        train_examples = self.create_training_examples()\n",
        "        \n",
        "        # ë°ì´í„°ë¡œë” ìƒì„±\n",
        "        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
        "        \n",
        "        # ì†ì‹¤ í•¨ìˆ˜\n",
        "        train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "        \n",
        "        # í›ˆë ¨ ì‹¤í–‰\n",
        "        model.fit(\n",
        "            train_objectives=[(train_dataloader, train_loss)],\n",
        "            epochs=epochs,\n",
        "            warmup_steps=int(len(train_dataloader) * 0.1),\n",
        "            show_progress_bar=True\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ\")\n",
        "        return model\n",
        "\n",
        "print(\"âœ… íŒŒì¸íŠœë‹ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
