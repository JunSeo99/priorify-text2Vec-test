import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import logging
import os
import argparse
import torch
from sklearn.metrics import precision_recall_fscore_support
import sys
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(PROJECT_ROOT)
from src.config import (
    BASE_MODEL_NAME, DATA_PATH, DEFAULT_OUTPUT_PATH, CATEGORIES_DEFINITIONS, 
    V2_IMPROVED_ENTITIES, NER_CONFIDENCE_THRESHOLD
)
from src.core.model_utils import ner_generalize_texts

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class AlgorithmEvaluator:
    """
    ë‹¤ì–‘í•œ ë²„ì „ì˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ í‰ê°€ í´ë˜ìŠ¤
    """
    def __init__(self, data_path, finetuned_model_path):
        self.data_path = data_path
        self.finetuned_model_path = finetuned_model_path
        
        self.base_model = SentenceTransformer(BASE_MODEL_NAME)
        self.finetuned_model = self._load_finetuned_model()
        
        self.categories = list(CATEGORIES_DEFINITIONS.keys())
        self.test_df = self._load_and_prepare_data()
        
        # ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©í•  ì¼ë°˜í™”ëœ ì œëª© ë¯¸ë¦¬ ê³„ì‚°
        self.test_df['generalized_title'] = ner_generalize_texts(self.test_df['title'].tolist())
        
        # V2 ê°œì„  ë²„ì „ë“¤ì„ ìœ„í•œ ì¼ë°˜í™”ëœ í…ìŠ¤íŠ¸ë“¤
        self.test_df['v2_improved_title'] = ner_generalize_texts(
            self.test_df['title'].tolist(), 
            entities_to_generalize=V2_IMPROVED_ENTITIES,
            confidence_threshold=NER_CONFIDENCE_THRESHOLD
        )
        self.test_df['v2_conservative_title'] = ner_generalize_texts(
            self.test_df['title'].tolist(), 
            entities_to_generalize=["PS"],  # ì¸ë¬¼ë§Œ ì¼ë°˜í™”
            confidence_threshold=0.7
        )
        
        # ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°
        self.simple_cat_embs = self.base_model.encode(self.categories, convert_to_tensor=True, normalize_embeddings=True)
        self.v3_cat_embs = self._get_keyword_avg_embs(self.base_model)
        self.v4_cat_embs = self._get_keyword_avg_embs(self.finetuned_model) if self.finetuned_model else None

    def _load_finetuned_model(self):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ. ì—†ìœ¼ë©´ None ë°˜í™˜."""
        if os.path.exists(self.finetuned_model_path):
            logging.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ: {self.finetuned_model_path}")
            return SentenceTransformer(self.finetuned_model_path)
        else:
            logging.warning(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.finetuned_model_path}")
            logging.warning("V4 í‰ê°€ëŠ” ê±´ë„ˆëœë‹ˆë‹¤. 'finetune.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
            return None
            
    def _load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ì…‹ ì¤€ë¹„."""
        df = pd.read_csv(self.data_path)
        # 'categories' ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° 'category'ë¡œ ëŒ€ì²´ ì‹œë„
        if 'categories' not in df.columns and 'category' in df.columns:
            df = df.rename(columns={'category': 'categories'})
            
        df.dropna(subset=['title', 'categories'], inplace=True)
        # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ ì¤‘ ì²« ë²ˆì§¸ ì¹´í…Œê³ ë¦¬ë¥¼ ì •ë‹µìœ¼ë¡œ ì‚¬ìš©
        df['category'] = df['categories'].apply(lambda x: x.split(';')[0].strip() if isinstance(x, str) else (x[0] if isinstance(x, list) and x else None))
        df.dropna(subset=['category'], inplace=True)

        # configì— ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
        original_len = len(df)
        df = df[df['category'].isin(self.categories)]
        new_len = len(df)
        if original_len > new_len:
            logging.warning(f"{original_len - new_len}ê°œì˜ ë°ì´í„°ê°€ configì— ì •ì˜ë˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì§€ê³  ìˆì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        logging.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {len(df)}ê°œ")
        return df.copy()
        
    def _get_keyword_avg_embs(self, model):
        """ì£¼ì–´ì§„ ëª¨ë¸ë¡œ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”© ê³„ì‚°."""
        category_embs = {}
        for category, keywords in CATEGORIES_DEFINITIONS.items():
            keyword_embs = model.encode(keywords, convert_to_numpy=True, normalize_embeddings=True)
            avg_emb = np.mean(keyword_embs, axis=0)
            if np.linalg.norm(avg_emb) > 0:
                avg_emb = avg_emb / np.linalg.norm(avg_emb)
            category_embs[category] = avg_emb.astype(np.float32)
        return torch.tensor(np.array(list(category_embs.values())), dtype=torch.float32).to(model.device)

    def _get_keyword_max_embs(self, model):
        """ì£¼ì–´ì§„ ëª¨ë¸ë¡œ ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ê°€ì¤‘í‰ê·  ì„ë² ë”© ê³„ì‚° (í‚¤ì›Œë“œë³„ TF-IDF ê°€ì¤‘ì¹˜ ì ìš©)"""
        category_embs = []
        for category in self.categories:
            keywords = CATEGORIES_DEFINITIONS[category]
            if not keywords:
                # í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš° 0ë²¡í„° ì‚¬ìš©
                category_embs.append(np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32))
                continue
                
            keyword_embs = model.encode(keywords, convert_to_numpy=True, normalize_embeddings=True)
            
            # í‚¤ì›Œë“œ ê¸¸ì´ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (ê¸´ í‚¤ì›Œë“œì¼ìˆ˜ë¡ ë” êµ¬ì²´ì ì´ë¯€ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜)
            weights = np.array([len(kw.split()) for kw in keywords], dtype=np.float32)
            weights = weights / np.sum(weights)  # ì •ê·œí™”
            
            # ê°€ì¤‘ í‰ê·  ê³„ì‚°
            weighted_emb = np.average(keyword_embs, axis=0, weights=weights)
            if np.linalg.norm(weighted_emb) > 0:
                weighted_emb = weighted_emb / np.linalg.norm(weighted_emb)
            category_embs.append(weighted_emb.astype(np.float32))
        return torch.tensor(np.array(category_embs), dtype=torch.float32).to(model.device)

    def _evaluate_version(self, version_name, model, title_column, category_embs):
        """ë‹¨ì¼ ë²„ì „ ì„±ëŠ¥ í‰ê°€ ë¡œì§. ë‹¤ì–‘í•œ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜."""
        logging.info(f"--- {version_name} í‰ê°€ ì‹œì‘ ---")
        title_embs = model.encode(self.test_df[title_column].tolist(), convert_to_tensor=True, normalize_embeddings=True)
        
        similarities = util.cos_sim(title_embs, category_embs)
        
        # ìƒìœ„ Kê°œ ì˜ˆì¸¡ ì¶”ì¶œ (K=5)
        top_k_preds = torch.topk(similarities, k=5, dim=1)
        pred_indices = top_k_preds.indices.cpu().numpy()
        
        true_categories = self.test_df['category'].tolist()
        # ì •ë‹µ ì¹´í…Œê³ ë¦¬ ì´ë¦„ -> ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        true_indices = [self.categories.index(cat) for cat in true_categories]
        
        # Hit Rate @1, @3, @5 ê³„ì‚°
        correct_at_1 = 0
        correct_at_3 = 0
        correct_at_5 = 0
        
        for i, true_idx in enumerate(true_indices):
            # @1: ì²« ë²ˆì§¸ ì˜ˆì¸¡ì´ ì •ë‹µì¸ ê²½ìš°
            if true_idx == pred_indices[i, 0]:
                correct_at_1 += 1
            # @3: ìƒìœ„ 3ê°œ ì˜ˆì¸¡ì— ì •ë‹µì´ í¬í•¨ëœ ê²½ìš°
            if true_idx in pred_indices[i, :3]:
                correct_at_3 += 1
            # @5: ìƒìœ„ 5ê°œ ì˜ˆì¸¡ì— ì •ë‹µì´ í¬í•¨ëœ ê²½ìš°
            if true_idx in pred_indices[i, :5]:
                correct_at_5 += 1

        total_count = len(self.test_df)
        hit_rate_1 = correct_at_1 / total_count
        hit_rate_3 = correct_at_3 / total_count
        hit_rate_5 = correct_at_5 / total_count

        # F1-score (macro) ê³„ì‚°
        top_1_predictions = [self.categories[idx] for idx in pred_indices[:, 0]]
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_categories, 
            top_1_predictions, 
            average='macro', 
            zero_division=0,
            labels=self.categories # ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ê³ ë ¤í•˜ë„ë¡ ëª…ì‹œ
        )
        
        metrics = {
            "Hit Rate @1": hit_rate_1,
            "Hit Rate @3": hit_rate_3,
            "Hit Rate @5": hit_rate_5,
            "F1 (macro)": f1,
        }
        
        logging.info(f"{version_name} í‰ê°€ ì™„ë£Œ: Hit@1={hit_rate_1:.2%}, Hit@3={hit_rate_3:.2%}, Hit@5={hit_rate_5:.2%}, F1={f1:.4f}")
        return metrics

    def _evaluate_simple_ensemble(self, version_name, text_column, embedding_configs, weight=0.5):
        """ë‹¨ìˆœ ì•™ìƒë¸” í‰ê°€ (2ê°œ êµ¬ì„±ìš”ì†Œ, 0.5:0.5 ê°€ì¤‘ì¹˜)"""
        logging.info(f"--- {version_name} ì•™ìƒë¸” í‰ê°€ ì‹œì‘ ---")
        
        all_similarities = []
        
        for config in embedding_configs:
            model = config['model']
            category_embs = config['category_embs']
            
            text_embs = model.encode(
                self.test_df[text_column].tolist(), 
                convert_to_tensor=True, 
                normalize_embeddings=True
            )
            similarities = util.cos_sim(text_embs, category_embs).cpu().numpy()
            all_similarities.append(similarities)
        
        # 0.5:0.5 ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”
        ensemble_similarities = (all_similarities[0] * weight + all_similarities[1] * weight)
        
        # ìƒìœ„ Kê°œ ì˜ˆì¸¡ ì¶”ì¶œ
        top_k_preds = np.argsort(ensemble_similarities, axis=1)[:, ::-1]
        
        true_categories = self.test_df['category'].tolist()
        true_indices = [self.categories.index(cat) for cat in true_categories]
        
        # Hit Rate ê³„ì‚°
        correct_at_1 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx == top_k_preds[i, 0])
        correct_at_3 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in top_k_preds[i, :3])
        correct_at_5 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in top_k_preds[i, :5])

        total_count = len(self.test_df)
        
        # F1-score ê³„ì‚°
        top_1_predictions = [self.categories[idx] for idx in top_k_preds[:, 0]]
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_categories, top_1_predictions, average='macro', zero_division=0, labels=self.categories
        )
        
        metrics = {
            "Hit Rate @1": correct_at_1 / total_count,
            "Hit Rate @3": correct_at_3 / total_count,
            "Hit Rate @5": correct_at_5 / total_count,
            "F1 (macro)": f1,
        }
        
        logging.info(f"{version_name} ì•™ìƒë¸” í‰ê°€ ì™„ë£Œ: Hit@1={metrics['Hit Rate @1']:.2%}, Hit@3={metrics['Hit Rate @3']:.2%}, Hit@5={metrics['Hit Rate @5']:.2%}, F1={f1:.4f}")
        return metrics

    def _ensemble_predict(self, ensemble_configs, text_column='generalized_title'):
        """ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰"""
        all_similarities = []
        weights = []
        
        for config in ensemble_configs:
            model_name = config['model']
            embedding_type = config['embedding']
            weight = config['weight']
            
            # ëª¨ë¸ ì„ íƒ
            if model_name == 'base':
                model = self.base_model
            elif model_name == 'finetuned' and self.finetuned_model:
                model = self.finetuned_model
            else:
                continue
                
            # ì„ë² ë”© ì„ íƒ
            if embedding_type == 'simple':
                category_embs = self.simple_cat_embs
            elif embedding_type == 'keyword_avg':
                if model_name == 'base':
                    category_embs = self.v3_cat_embs
                else:
                    category_embs = self.v4_cat_embs
            elif embedding_type == 'keyword_max':
                # í‚¤ì›Œë“œë³„ ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚°
                text_embs = model.encode(self.test_df[text_column].tolist(), convert_to_tensor=True, normalize_embeddings=True)
                similarities = self._calculate_keyword_max_similarities(text_embs, model)
                all_similarities.append(similarities)
                weights.append(weight)
                continue
            else:
                continue
                
            # ì¼ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
            text_embs = model.encode(self.test_df[text_column].tolist(), convert_to_tensor=True, normalize_embeddings=True)
            similarities = util.cos_sim(text_embs, category_embs).cpu().numpy()
            all_similarities.append(similarities)
            weights.append(weight)
        
        if not all_similarities:
            return None
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸”
        weights = np.array(weights)
        weights = weights / np.sum(weights)  # ì •ê·œí™”
        
        ensemble_similarities = np.zeros_like(all_similarities[0])
        for sim, weight in zip(all_similarities, weights):
            ensemble_similarities += sim * weight
        
        return ensemble_similarities

    def _calculate_keyword_max_similarities(self, text_embs, model):
        """í‚¤ì›Œë“œë³„ ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚°"""
        similarities = []
        
        for text_emb in text_embs:
            text_similarities = []
            for category in self.categories:
                keywords = CATEGORIES_DEFINITIONS[category]
                keyword_embs = model.encode(keywords, convert_to_tensor=True, normalize_embeddings=True)
                # ê° í‚¤ì›Œë“œì™€ì˜ ìœ ì‚¬ë„ ì¤‘ ìµœëŒ€ê°’
                max_sim = torch.max(util.cos_sim(text_emb.unsqueeze(0), keyword_embs)).item()
                text_similarities.append(max_sim)
            similarities.append(text_similarities)
        
        return np.array(similarities)

    def _evaluate_ensemble(self, ensemble_configs, version_name, text_column='generalized_title'):
        """ì•™ìƒë¸” ë²„ì „ í‰ê°€"""
        logging.info(f"--- {version_name} ì•™ìƒë¸” í‰ê°€ ì‹œì‘ ---")
        
        similarities = self._ensemble_predict(ensemble_configs, text_column)
        if similarities is None:
            return {"Hit Rate @1": 0, "Hit Rate @3": 0, "Hit Rate @5": 0, "F1 (macro)": 0}
        
        # ìƒìœ„ Kê°œ ì˜ˆì¸¡ ì¶”ì¶œ
        top_k_preds = np.argsort(similarities, axis=1)[:, ::-1]
        
        true_categories = self.test_df['category'].tolist()
        true_indices = [self.categories.index(cat) for cat in true_categories]
        
        # Hit Rate ê³„ì‚°
        correct_at_1 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx == top_k_preds[i, 0])
        correct_at_3 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in top_k_preds[i, :3])
        correct_at_5 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in top_k_preds[i, :5])

        total_count = len(self.test_df)
        
        # F1-score ê³„ì‚°
        top_1_predictions = [self.categories[idx] for idx in top_k_preds[:, 0]]
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_categories, top_1_predictions, average='macro', zero_division=0, labels=self.categories
        )
        
        metrics = {
            "Hit Rate @1": correct_at_1 / total_count,
            "Hit Rate @3": correct_at_3 / total_count,
            "Hit Rate @5": correct_at_5 / total_count,
            "F1 (macro)": f1,
        }
        
        logging.info(f"{version_name} ì•™ìƒë¸” í‰ê°€ ì™„ë£Œ: Hit@1={metrics['Hit Rate @1']:.2%}")
        return metrics

    def optimize_ensemble_hyperparameters(self):
        """ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        logging.info("ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        # ê°€ëŠ¥í•œ ì•™ìƒë¸” ì¡°í•©ë“¤
        ensemble_combinations = [
            # V3 ê¸°ë°˜ ì•™ìƒë¸”ë“¤
            [
                {'model': 'base', 'embedding': 'simple', 'weight': 0.3},
                {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.7}
            ],
            [
                {'model': 'base', 'embedding': 'simple', 'weight': 0.4},
                {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.6}
            ],
            [
                {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.6},
                {'model': 'base', 'embedding': 'keyword_max', 'weight': 0.4}
            ],
            [
                {'model': 'base', 'embedding': 'simple', 'weight': 0.2},
                {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.6},
                {'model': 'base', 'embedding': 'keyword_max', 'weight': 0.2}
            ],
        ]
        
        # V4ê°€ ìˆìœ¼ë©´ íŒŒì¸íŠœë‹ ëª¨ë¸ ì¡°í•©ë„ ì¶”ê°€
        if self.finetuned_model:
            v4_combinations = [
                [
                    {'model': 'finetuned', 'embedding': 'simple', 'weight': 0.3},
                    {'model': 'finetuned', 'embedding': 'keyword_avg', 'weight': 0.7}
                ],
                [
                    {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.4},
                    {'model': 'finetuned', 'embedding': 'keyword_avg', 'weight': 0.6}
                ],
                [
                    {'model': 'base', 'embedding': 'simple', 'weight': 0.2},
                    {'model': 'base', 'embedding': 'keyword_avg', 'weight': 0.3},
                    {'model': 'finetuned', 'embedding': 'keyword_avg', 'weight': 0.5}
                ],
            ]
            ensemble_combinations.extend(v4_combinations)
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í›„ë³´ë“¤
        text_columns = ['title', 'generalized_title', 'v2_improved_title']
        
        best_results = {}
        all_results = []
        
        for text_col in text_columns:
            for i, ensemble_config in enumerate(ensemble_combinations):
                config_name = f"ensemble_{i+1}_{text_col}"
                result = self._evaluate_ensemble(ensemble_config, config_name, text_col)
                result['config'] = ensemble_config
                result['text_column'] = text_col
                result['config_name'] = config_name
                all_results.append(result)
        
        # ê° ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        for metric in ["Hit Rate @1", "Hit Rate @3", "Hit Rate @5"]:
            best_result = max(all_results, key=lambda x: x[metric])
            best_results[metric] = best_result
            
        return best_results, all_results

    def run_all_and_compare(self):
        """ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë²„ì „ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì¶œë ¥."""
        results = {}
        
        # V1: ë‹¨ìˆœ ì„ë² ë”© (NER ì—†ìŒ + ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª…)
        results['V1 (ë‹¨ìˆœ ì„ë² ë”©)'] = self._evaluate_version(
            "V1", self.base_model, 'title', self.simple_cat_embs
        )
        
        # V2: NER + ì•™ìƒë¸” (ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª… + í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”©ì„ 0.5:0.5ë¡œ ì•™ìƒë¸”)
        v2_configs = [
            {'model': self.base_model, 'category_embs': self.simple_cat_embs},
            {'model': self.base_model, 'category_embs': self.v3_cat_embs}
        ]
        results['V2 (NER + ì•™ìƒë¸”)'] = self._evaluate_simple_ensemble(
            "V2", 'generalized_title', v2_configs
        )
        
        # V3: NER + ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œí™” + ì•™ìƒë¸” (í‚¤ì›Œë“œ í‰ê·  + í‚¤ì›Œë“œ ìµœëŒ€ê°’ì„ 0.5:0.5ë¡œ ì•™ìƒë¸”)
        v3_configs = [
            {'model': self.base_model, 'category_embs': self.v3_cat_embs},
            {'model': self.base_model, 'category_embs': self._get_keyword_max_embs(self.base_model)}
        ]
        results['V3 (NER + í‚¤ì›Œë“œí™” + ì•™ìƒë¸”)'] = self._evaluate_simple_ensemble(
            "V3", 'generalized_title', v3_configs
        )
        
        # V4: íŒŒì¸íŠœë‹ + ì•™ìƒë¸” (íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª… + í‚¤ì›Œë“œ í‰ê· ì„ 0.5:0.5ë¡œ ì•™ìƒë¸”)
        if os.path.exists(os.path.join(PROJECT_ROOT, 'models/finetuned_ensemble_v4')):
            logging.info("íŒŒì¸íŠœë‹ëœ ì•™ìƒë¸” ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            try:
                finetuned_model = SentenceTransformer(os.path.join(PROJECT_ROOT, 'models/finetuned_ensemble_v4'))
                
                # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”©
                finetuned_keyword_embs = self._get_keyword_avg_embs(finetuned_model)
                
                # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ì•™ìƒë¸” êµ¬ì„± (ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª… + í‚¤ì›Œë“œ í‰ê· )
                v4_configs = [
                    {'model': finetuned_model, 'category_embs': torch.tensor(
                        finetuned_model.encode(self.categories, convert_to_numpy=True, normalize_embeddings=True)
                    ).to(finetuned_model.device)},
                    {'model': finetuned_model, 'category_embs': finetuned_keyword_embs}
                ]
                results['V4 (íŒŒì¸íŠœë‹ + ì•™ìƒë¸”)'] = self._evaluate_simple_ensemble(
                    "V4", 'generalized_title', v4_configs
                )
            except Exception as e:
                logging.warning(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                results['V4 (íŒŒì¸íŠœë‹ + ì•™ìƒë¸”)'] = {'hit_rate@1': 0, 'hit_rate@3': 0, 'f1_score': 0}
        else:
            logging.warning("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € finetune.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            results['V4 (íŒŒì¸íŠœë‹ + ì•™ìƒë¸”)'] = {'hit_rate@1': 0, 'hit_rate@3': 0, 'f1_score': 0}

        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print(" " * 25 + "ì•Œê³ ë¦¬ì¦˜ ë²„ì „ë³„ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*80)
        
        headers = ["Algorithm Version", "Hit Rate @1", "Hit Rate @3", "Hit Rate @5", "F1 (macro)"]
        col_widths = [35, 15, 15, 15, 12]
        header_line = " | ".join([f"{h:<{w}}" for h, w in zip(headers, col_widths)])
        print(header_line)
        print("-" * len(header_line))

        for version, scores in results.items():
            row_data = [version]
            for header in headers[1:]:
                score = scores.get(header)
                if isinstance(score, float):
                    if "Hit Rate" in header:
                        display_score = f"{score:.2%}"
                    else:
                        display_score = f"{score:.4f}"
                else:
                    display_score = str(score)
                row_data.append(display_score)
            
            row_line = " | ".join([f"{d:<{w}}" for d, w in zip(row_data, col_widths)])
            print(row_line)
        print("="*80)

    def _print_ensemble_optimization_results(self, best_ensemble_configs):
        """ì•™ìƒë¸” ìµœì í™” ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print(" " * 20 + "ğŸ”§ ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê²°ê³¼")
        print("="*80)
        
        for metric, config_info in best_ensemble_configs.items():
            print(f"\nğŸ“Š {metric} ìµœì  ì„¤ì •:")
            print(f"   ì„±ëŠ¥: {config_info[metric]:.4f}")
            print(f"   í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {config_info['text_column']}")
            print(f"   ì•™ìƒë¸” êµ¬ì„±:")
            
            for i, component in enumerate(config_info['config']):
                print(f"     {i+1}. ëª¨ë¸: {component['model']} | ì„ë² ë”©: {component['embedding']} | ê°€ì¤‘ì¹˜: {component['weight']}")
        
        print("="*80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë‹¤ì–‘í•œ ë²„ì „ì˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ì„ ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤.")
    parser.add_argument("--data_path", type=str, default=DATA_PATH, help="í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--finetuned_model_path", type=str, default=DEFAULT_OUTPUT_PATH, help="V4 í‰ê°€ì— ì‚¬ìš©í•  íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ê²½ë¡œ")
    args = parser.parse_args()
    if not os.path.isabs(args.finetuned_model_path):
        args.finetuned_model_path = os.path.join(PROJECT_ROOT, args.finetuned_model_path)
    if not os.path.isabs(args.data_path):
        args.data_path = os.path.join(PROJECT_ROOT, args.data_path)

    evaluator = AlgorithmEvaluator(data_path=args.data_path, finetuned_model_path=args.finetuned_model_path)
    evaluator.run_all_and_compare()
