import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import logging
import os
import argparse
import torch
import sys
from itertools import product

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(PROJECT_ROOT)
from src.config import (
    BASE_MODEL_NAME, DATA_PATH, DEFAULT_OUTPUT_PATH, CATEGORIES_DEFINITIONS
)
from src.core.model_utils import ner_generalize_texts

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class EnsembleEvaluator:
    """
    V3, V4ì— ì•™ìƒë¸” ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, data_path, finetuned_model_path):
        self.data_path = data_path
        self.finetuned_model_path = finetuned_model_path
        
        # ëª¨ë¸ ë¡œë“œ
        self.base_model = SentenceTransformer(BASE_MODEL_NAME)
        self.finetuned_model = self._load_finetuned_model()
        
        self.categories = list(CATEGORIES_DEFINITIONS.keys())
        self.test_df = self._load_and_prepare_data()
        
        # ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤€ë¹„
        self._prepare_text_variants()
        
        # ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ë°©ë²•ë“¤ ì¤€ë¹„
        self._prepare_category_embeddings()
        
        logging.info(f"ì•™ìƒë¸” í‰ê°€ ì¤€ë¹„ ì™„ë£Œ: í…ŒìŠ¤íŠ¸ ë°ì´í„° {len(self.test_df)}ê°œ")

    def _load_finetuned_model(self):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(self.finetuned_model_path):
            logging.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ: {self.finetuned_model_path}")
            return SentenceTransformer(self.finetuned_model_path)
        else:
            logging.warning(f"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {self.finetuned_model_path}")
            return None
            
    def _load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        df = pd.read_csv(self.data_path)
        
        # 'categories' ì»¬ëŸ¼ ì²˜ë¦¬
        if 'categories' not in df.columns and 'category' in df.columns:
            df = df.rename(columns={'category': 'categories'})
            
        df.dropna(subset=['title', 'categories'], inplace=True)
        df['category'] = df['categories'].apply(
            lambda x: x.split(';')[0].strip() if isinstance(x, str) else (x[0] if isinstance(x, list) and x else None)
        )
        df.dropna(subset=['category'], inplace=True)

        # configì— ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§
        df = df[df['category'].isin(self.categories)]
        
        return df.copy()
        
    def _prepare_text_variants(self):
        """ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë³€í˜• ì¤€ë¹„"""
        logging.info("í…ìŠ¤íŠ¸ ë³€í˜• ì¤€ë¹„ ì¤‘...")
        
        # 1. ì›ë³¸ í…ìŠ¤íŠ¸
        self.test_df['text_original'] = self.test_df['title']
        
        # 2. NER ì ìš© í…ìŠ¤íŠ¸
        self.test_df['text_ner'] = ner_generalize_texts(self.test_df['title'].tolist())
        
        # 3. ë³´ìˆ˜ì  NER (ì¸ë¬¼ë§Œ)
        self.test_df['text_ner_conservative'] = ner_generalize_texts(
            self.test_df['title'].tolist(), 
            entities_to_generalize=["PS"],
            confidence_threshold=0.7
        )
        
    def _prepare_category_embeddings(self):
        """ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ë°©ë²• ì¤€ë¹„"""
        logging.info("ì¹´í…Œê³ ë¦¬ ì„ë² ë”© ì¤€ë¹„ ì¤‘...")
        
        self.category_embeddings = {}
        
        # ë°©ë²• 1: ë‹¨ìˆœ ì¹´í…Œê³ ë¦¬ëª… ì„ë² ë”©
        self.category_embeddings['simple_base'] = self.base_model.encode(
            self.categories, convert_to_numpy=True, normalize_embeddings=True
        )
        if self.finetuned_model:
            self.category_embeddings['simple_finetuned'] = self.finetuned_model.encode(
                self.categories, convert_to_numpy=True, normalize_embeddings=True
            )
        
        # ë°©ë²• 2: í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”©
        self.category_embeddings['keyword_avg_base'] = self._get_keyword_avg_embs(self.base_model)
        if self.finetuned_model:
            self.category_embeddings['keyword_avg_finetuned'] = self._get_keyword_avg_embs(self.finetuned_model)
            
        # ë°©ë²• 3: í‚¤ì›Œë“œ ê°œë³„ ì„ë² ë”© (ìµœëŒ€ ìœ ì‚¬ë„ìš©)
        self.category_keyword_embs_base = self._get_keyword_individual_embs(self.base_model)
        if self.finetuned_model:
            self.category_keyword_embs_finetuned = self._get_keyword_individual_embs(self.finetuned_model)

    def _get_keyword_avg_embs(self, model):
        """í‚¤ì›Œë“œ í‰ê·  ì„ë² ë”© ê³„ì‚°"""
        category_embs = []
        for category in self.categories:
            keywords = CATEGORIES_DEFINITIONS[category]
            keyword_embs = model.encode(keywords, convert_to_numpy=True, normalize_embeddings=True)
            avg_emb = np.mean(keyword_embs, axis=0)
            if np.linalg.norm(avg_emb) > 0:
                avg_emb = avg_emb / np.linalg.norm(avg_emb)
            category_embs.append(avg_emb)
        return np.array(category_embs)
    
    def _get_keyword_individual_embs(self, model):
        """í‚¤ì›Œë“œë³„ ê°œë³„ ì„ë² ë”© ê³„ì‚°"""
        category_keyword_embs = {}
        for category in self.categories:
            keywords = CATEGORIES_DEFINITIONS[category]
            keyword_embs = model.encode(keywords, convert_to_numpy=True, normalize_embeddings=True)
            category_keyword_embs[category] = keyword_embs
        return category_keyword_embs

    def _calculate_similarities(self, text_column, model, embedding_method):
        """ìœ ì‚¬ë„ ê³„ì‚°"""
        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        text_embs = model.encode(
            self.test_df[text_column].tolist(), 
            convert_to_numpy=True, 
            normalize_embeddings=True
        )
        
        if embedding_method.startswith('keyword_max'):
            # í‚¤ì›Œë“œë³„ ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚°
            model_suffix = embedding_method.split('_')[-1]
            if model_suffix == 'base':
                category_keyword_embs = self.category_keyword_embs_base
            else:
                category_keyword_embs = self.category_keyword_embs_finetuned
                
            similarities = []
            for text_emb in text_embs:
                text_similarities = []
                for category in self.categories:
                    keyword_embs = category_keyword_embs[category]
                    # ê° í‚¤ì›Œë“œì™€ì˜ ìœ ì‚¬ë„ ì¤‘ ìµœëŒ€ê°’
                    max_sim = np.max(util.cos_sim([text_emb], torch.tensor(keyword_embs)).cpu().numpy())
                    text_similarities.append(max_sim)
                similarities.append(text_similarities)
            return np.array(similarities)
        else:
            # í‰ê·  ì„ë² ë”©ê³¼ ìœ ì‚¬ë„ ê³„ì‚°
            category_embs = self.category_embeddings[embedding_method]
            return util.cos_sim(text_embs, torch.tensor(category_embs)).cpu().numpy()

    def _ensemble_predict(self, ensemble_config):
        """ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰"""
        all_similarities = []
        weights = []
        
        for component in ensemble_config:
            text_column = component['text_column']
            model_name = component['model']
            embedding_method = component['embedding_method']
            weight = component['weight']
            
            # ëª¨ë¸ ì„ íƒ
            if model_name == 'base':
                model = self.base_model
            elif model_name == 'finetuned' and self.finetuned_model:
                model = self.finetuned_model
            else:
                continue
                
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = self._calculate_similarities(text_column, model, embedding_method)
            all_similarities.append(similarities)
            weights.append(weight)
        
        if not all_similarities:
            return None
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸”
        weights = np.array(weights)
        weights = weights / np.sum(weights)  # ì •ê·œí™”
        
        ensemble_similarities = np.zeros_like(all_similarities[0])
        for sim, weight in zip(all_similarities, weights):
            ensemble_similarities += sim * weight
        
        return ensemble_similarities

    def _evaluate_predictions(self, similarities):
        """ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€"""
        if similarities is None:
            return None
            
        # Top-k ì˜ˆì¸¡
        top_k_indices = np.argsort(similarities, axis=1)[:, ::-1]
        
        true_categories = self.test_df['category'].tolist()
        true_indices = [self.categories.index(cat) for cat in true_categories]
        
        # Hit Rate ê³„ì‚°
        correct_at_1 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx == top_k_indices[i, 0])
        correct_at_3 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in top_k_indices[i, :3])
        correct_at_5 = sum(1 for i, true_idx in enumerate(true_indices) if true_idx in top_k_indices[i, :5])

        total_count = len(self.test_df)
        
        return {
            "Hit Rate @1": correct_at_1 / total_count,
            "Hit Rate @3": correct_at_3 / total_count,
            "Hit Rate @5": correct_at_5 / total_count,
        }

    def optimize_ensemble_hyperparameters(self):
        """ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        logging.info("ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
        
        # ìµœì í™”í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤
        text_columns = ['text_original', 'text_ner', 'text_ner_conservative']
        embedding_methods = ['simple_base', 'keyword_avg_base', 'keyword_max_base']
        
        if self.finetuned_model:
            embedding_methods.extend(['simple_finetuned', 'keyword_avg_finetuned', 'keyword_max_finetuned'])
        
        # ê°€ì¤‘ì¹˜ ì¡°í•©ë“¤
        weight_combinations = [
            [1.0],  # ë‹¨ì¼ ë°©ë²•
            [0.7, 0.3],  # ë‘ ë°©ë²• ì¡°í•©
            [0.5, 0.5],
            [0.3, 0.7],
            [0.6, 0.25, 0.15],  # ì„¸ ë°©ë²• ì¡°í•©
            [0.5, 0.3, 0.2],
            [0.4, 0.4, 0.2],
        ]
        
        best_configs = []
        all_results = []
        
        # ë‹¨ì¼ ë°©ë²• í…ŒìŠ¤íŠ¸
        for text_col in text_columns:
            for emb_method in embedding_methods:
                config = [{
                    'text_column': text_col,
                    'model': emb_method.split('_')[-1],
                    'embedding_method': emb_method,
                    'weight': 1.0
                }]
                
                similarities = self._ensemble_predict(config)
                result = self._evaluate_predictions(similarities)
                
                if result:
                    result['config'] = config
                    result['config_name'] = f"{text_col}_{emb_method}"
                    all_results.append(result)
        
        # ì•™ìƒë¸” ì¡°í•© í…ŒìŠ¤íŠ¸ (ìƒìœ„ ì„±ëŠ¥ ë°©ë²•ë“¤ë¡œë§Œ)
        # Hit Rate @1 ê¸°ì¤€ ìƒìœ„ 5ê°œ ë°©ë²• ì„ íƒ
        top_single_methods = sorted(all_results, key=lambda x: x["Hit Rate @1"], reverse=True)[:5]
        
        logging.info(f"ìƒìœ„ 5ê°œ ë‹¨ì¼ ë°©ë²•ìœ¼ë¡œ ì•™ìƒë¸” ì¡°í•© í…ŒìŠ¤íŠ¸...")
        
        for i, method1 in enumerate(top_single_methods):
            for j, method2 in enumerate(top_single_methods[i+1:], i+1):
                for weights in weight_combinations[1:3]:  # ë‘ ë°©ë²• ì¡°í•©ë§Œ
                    config = [
                        {**method1['config'][0], 'weight': weights[0]},
                        {**method2['config'][0], 'weight': weights[1]}
                    ]
                    
                    similarities = self._ensemble_predict(config)
                    result = self._evaluate_predictions(similarities)
                    
                    if result:
                        result['config'] = config
                        result['config_name'] = f"ensemble_{method1['config_name']}_{method2['config_name']}"
                        all_results.append(result)
        
        # ìµœê³  ì„±ëŠ¥ ì„¤ì •ë“¤ ì°¾ê¸°
        best_hit_1 = max(all_results, key=lambda x: x["Hit Rate @1"])
        best_hit_3 = max(all_results, key=lambda x: x["Hit Rate @3"])
        best_hit_5 = max(all_results, key=lambda x: x["Hit Rate @5"])
        
        return {
            "Hit Rate @1": best_hit_1,
            "Hit Rate @3": best_hit_3,
            "Hit Rate @5": best_hit_5
        }, all_results

    def evaluate_v3_v4_ensemble(self):
        """V3, V4ì— ìµœì  ì•™ìƒë¸” ì ìš©í•˜ì—¬ í‰ê°€"""
        logging.info("V3, V4 ì•™ìƒë¸” ìµœì í™” ë° í‰ê°€ ì‹œì‘...")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
        best_configs, all_results = self.optimize_ensemble_hyperparameters()
        
        # V3 ì•™ìƒë¸” (Base ëª¨ë¸ + í‚¤ì›Œë“œ ê¸°ë°˜)
        v3_results = {}
        best_config = best_configs["Hit Rate @1"]['config']
        
        # V3: Base ëª¨ë¸ë¡œë§Œ êµ¬ì„±ëœ ì•™ìƒë¸”
        v3_config = []
        for component in best_config:
            if component['model'] == 'base':
                v3_config.append(component)
        
        if v3_config:
            similarities = self._ensemble_predict(v3_config)
            v3_result = self._evaluate_predictions(similarities)
            if v3_result:
                v3_results['V3 (Base+ì•™ìƒë¸”)'] = v3_result
        
        # V4 ì•™ìƒë¸” (Finetuned ëª¨ë¸ í¬í•¨)
        v4_results = {}
        if self.finetuned_model:
            # V4-1: Finetuned ëª¨ë¸ë¡œë§Œ êµ¬ì„±ëœ ì•™ìƒë¸”
            v4_config = []
            for component in best_config:
                if component['model'] == 'finetuned':
                    v4_config.append(component)
            
            if v4_config:
                similarities = self._ensemble_predict(v4_config)
                v4_result = self._evaluate_predictions(similarities)
                if v4_result:
                    v4_results['V4 (Finetuned+ì•™ìƒë¸”)'] = v4_result
            
            # V4-2: Base + Finetuned í˜¼í•© ì•™ìƒë¸”
            mixed_similarities = self._ensemble_predict(best_config)
            mixed_result = self._evaluate_predictions(mixed_similarities)
            if mixed_result:
                v4_results['V4 (í˜¼í•©+ì•™ìƒë¸”)'] = mixed_result
        
        return best_configs, v3_results, v4_results, all_results

    def run_evaluation(self):
        """ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        best_configs, v3_results, v4_results, all_results = self.evaluate_v3_v4_ensemble()
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_results(best_configs, v3_results, v4_results)
        
        return best_configs, v3_results, v4_results

    def _print_results(self, best_configs, v3_results, v4_results):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*100)
        print(" " * 30 + "V3, V4 ì•™ìƒë¸” ê¸°ë²• ì ìš© ê²°ê³¼")
        print("="*100)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
        print("\nğŸ“Š ìµœì  ì•™ìƒë¸” í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for metric, config_info in best_configs.items():
            print(f"\n{metric} ìµœì  ì„¤ì •:")
            print(f"  - ì„±ëŠ¥: {config_info[metric]:.4f}")
            print(f"  - ì„¤ì •: {config_info['config_name']}")
            
            for i, component in enumerate(config_info['config']):
                print(f"  - ì»´í¬ë„ŒíŠ¸ {i+1}: {component['text_column']} + {component['embedding_method']} (ê°€ì¤‘ì¹˜: {component['weight']})")
        
        # V3 ê²°ê³¼
        if v3_results:
            print(f"\nğŸš€ V3 ì•™ìƒë¸” ê²°ê³¼:")
            headers = ["Version", "Hit Rate @1", "Hit Rate @3", "Hit Rate @5"]
            col_widths = [25, 15, 15, 15]
            header_line = " | ".join([f"{h:<{w}}" for h, w in zip(headers, col_widths)])
            print(header_line)
            print("-" * len(header_line))
            
            for version, scores in v3_results.items():
                row_data = [version]
                for header in headers[1:]:
                    score = scores.get(header, 0)
                    row_data.append(f"{score:.2%}")
                
                row_line = " | ".join([f"{d:<{w}}" for d, w in zip(row_data, col_widths)])
                print(row_line)
        
        # V4 ê²°ê³¼
        if v4_results:
            print(f"\nğŸ”¥ V4 ì•™ìƒë¸” ê²°ê³¼:")
            headers = ["Version", "Hit Rate @1", "Hit Rate @3", "Hit Rate @5"]
            col_widths = [25, 15, 15, 15]
            header_line = " | ".join([f"{h:<{w}}" for h, w in zip(headers, col_widths)])
            print(header_line)
            print("-" * len(header_line))
            
            for version, scores in v4_results.items():
                row_data = [version]
                for header in headers[1:]:
                    score = scores.get(header, 0)
                    row_data.append(f"{score:.2%}")
                
                row_line = " | ".join([f"{d:<{w}}" for d, w in zip(row_data, col_widths)])
                print(row_line)
        
        print("="*100)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="V3, V4ì— ì•™ìƒë¸” ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€")
    parser.add_argument("--data_path", type=str, default=DATA_PATH, help="ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--finetuned_model_path", type=str, default=DEFAULT_OUTPUT_PATH, help="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    if not os.path.isabs(args.finetuned_model_path):
        args.finetuned_model_path = os.path.join(PROJECT_ROOT, args.finetuned_model_path)
    if not os.path.isabs(args.data_path):
        args.data_path = os.path.join(PROJECT_ROOT, args.data_path)

    evaluator = EnsembleEvaluator(
        data_path=args.data_path,
        finetuned_model_path=args.finetuned_model_path
    )
    
    best_configs, v3_results, v4_results = evaluator.run_evaluation() 